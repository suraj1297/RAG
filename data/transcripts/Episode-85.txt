0.125 [bright music]
0.958 - Welcome to the "Huberman Lab Podcast,"
2.33 where we discuss science
and science-based tools
4.93 for everyday life.
9.35 I'm Andrew Huberman,
10.46 and I'm a Professor of
Neurobiology and Ophthalmology
13.27 at Stanford School of Medicine.
15.06 Today I have the pleasure of
introducing Dr. Lex Fridman
17.95 as our guest on the
"Huberman Lab Podcast."
21.15 Dr. Fridman is a researcher
at MIT specializing
23.92 in machine learning,
25.16 artificial intelligence and
human robot interactions.
29.36 I must say that the conversation with Lex
31.85 was without question,
33.83 one of the most fascinating conversations
35.7 that I've ever had,
36.64 not just in my career, but in my lifetime.
39.131 I knew that Lex worked on these topics.
41.55 And I think many of you are
probably familiar with Lex
43.95 and his interest in these topics
45.24 from his incredible podcast,
the "Lex Fridman Podcast."
48.29 If you're not already
watching that podcast,
50.28 please subscribe to it.
51.18 It is absolutely fantastic.
53.76 But in holding this conversation with Lex,
55.5 I realized something far more important.
58.327 He revealed to us a bit of his dream.
60.95 His dream about humans and robots,
63.4 about humans and machines,
65.02 and about how those interactions
66.63 can change the way that
we perceive ourselves
68.92 and that we interact with the world.
70.8 We discuss relationships of all kinds,
73.1 relationships with animals,
74.75 relationships with friends,
76.47 relationships with family
and romantic relationships.
80.46 And we discuss relationships
with the machines.
83.23 Machines that move and
machines that don't move,
86.224 and machines that come
to understand us in ways
89.02 that we could never
understand for ourselves,
91.73 and how those machines can
educate us about ourselves.
95.77 Before this conversation,
96.94 I had no concept of the ways
99.42 in which machines could inform me
101.38 or anyone about themselves.
103.74 By the end,
104.62 I was absolutely taken with the idea,
106.82 and I'm still taken with the idea
108.58 that interactions with machines
109.98 have a very particular kind,
111.97 a kind that Lex understands and
wants to bring to the world,
115.21 can not only transform the self,
116.972 but may very well transform humanity.
120.14 So whether or not you're familiar
121.39 with Dr. Lex Fridman or not,
123.35 I'm certain you're going to learn
124.42 a tremendous amount from him
125.89 during the course of our discussion,
127.55 and that it will transform
128.73 the way that you think about
yourself and about the world.
132.09 Before we begin,
133.21 I want to mention that this podcast
134.69 is separate from my teaching
and research roles at Stanford.
137.58 It is however part of my desire and effort
139.632 to bring zero cost to consumer
information about science
142.77 and science-related tools
to the general public.
145.6 In keeping with that theme,
146.75 I'd like to thank the
sponsors of today's podcast.
149.63 Our first sponsor is ROKA.
151.331 ROKA makes sunglasses and eyeglasses
153.38 that are of absolutely phenomenal quality.
155.67 The company was founded
156.53 by two All-American
swimmers from Stanford,
158.53 and everything about the sunglasses
160.2 and eyeglasses they've designed
had performance in mind.
163.95 I've spent a career working
on the visual system.
166.26 And one of the fundamental issues
167.73 that your visual system has to deal with
169.74 is how to adjust what you see
171.96 when it gets darker or
brighter in your environment.
174.64 With ROKA Sunglasses and Eyeglasses,
176.86 whether or not it's dim
in the room or outside,
179.14 or not there's cloud cover,
180.19 or whether or not you walk into a shadow,
181.59 you can always see the
world with absolute clarity.
184.32 And that just tells me
that they really understand
186.26 the way that the visual system works.
187.65 Processes like habituation
and attenuation.
190.18 All these things that work
at a real mechanistic level
192.56 have been built into these glasses.
194.59 In addition, the glasses
are very lightweight.
196.79 You don't even notice really
that they're on your face.
199.11 And the quality of the lenses is terrific.
201.8 Now, the glasses were also designed
203.5 so that you could use them,
204.41 not just while working
or at dinner, et cetera,
206.86 but while exercising.
208.59 They don't fall off your
face or slip off your face
210.62 if you're sweating.
211.81 And as I mentioned,
212.643 they're extremely lightweight.
213.476 So you can use them while running,
214.86 you can use them while
cycling and so forth.
217.07 Also the aesthetic of
ROKA glasses is terrific.
219.69 Unlike a lot of performance
glasses out there,
221.69 which frankly make
people look like cyborgs,
224.286 these glasses look great.
226.03 You can wear them out to dinner,
227.21 you can wear them for
essentially any occasion.
230.48 If you'd like to try ROKA glasses,
232.01 you can go to roka.com.
233.66 That's R-O-K-A .com and
enter the code Huberman
236.9 to save 20% off your first order.
239.2 That's ROKA,
240.033 R-O-K-A .com and enter the
code Huberman at checkout.
243.54 Today's episode is also
brought to us by InsideTracker.
246.899 InsideTracker is a
personalized nutrition platform
249.43 that analyzes data from your blood and DNA
252.01 to help you better understand your body
253.61 and help you reach your health goals.
255.74 I'm a big believer in getting
regular blood work done
258.25 for the simple reason
that many of the factors
260.74 that impact our immediate
and long-term health
263.21 can only be assessed from
a quality blood test.
265.87 And now with the advent
of quality DNA tests,
268.5 we can also get insight
269.386 into some of our genetic underpinnings
271.78 of our current and long-term health.
274.019 The problem with a lot of blood
275.61 and DNA tests out there, however,
277.47 is you get the data back and
you don't know what to do
279.41 with those data.
280.51 You see that certain things are high
281.81 or certain things are low,
283.03 but you really don't know
what the actionable items are,
285.24 what to do with all that information.
287.33 With InsideTracker,
288.69 they make it very easy to
act in the appropriate ways
291.69 on the information that you get back
293.29 from those blood and DNA tests.
295.11 And that's through the use
of their online platform.
297.63 They have a really easy to use dashboard
299.91 that tells you what sorts of
things can bring the numbers
302.529 for your metabolic factors,
endocrine factors, et cetera,
305.77 into the ranges that you want and need
308.07 for immediate and long-term health.
310.19 In fact, I know one individual
just by way of example,
313.13 that was feeling good,
314.27 but decided to go with
an InsideTracker test
316.5 and discovered that they had high levels
318.0 of what's called C-reactive protein.
319.85 They would have never
detected that otherwise.
321.85 C-reactive protein is associated
323.39 with a number of deleterious
health conditions,
325.97 some heart issues, eye issues, et cetera.
328.24 And so they were able
to take immediate action
330.22 to try and resolve those CRP levels.
333.42 And so with InsideTracker,
334.66 you get that sort of insight.
336.02 And as I mentioned before,
337.06 without a blood or DNA test,
338.48 there's no way you're going
to get that sort of insight
340.41 until symptoms start to show up.
342.86 If you'd like to try InsideTracker,
344.35 you can go to insidetracker.com/huberman
347.4 to get 25% off any of
InsideTracker's plans.
350.29 You just use the code
Huberman at checkout.
352.72 That's insidetracker.com/huberman
355.45 to get 25% off any of
InsideTracker's plans.
358.73 Today's podcast is brought
to us by Athletic Greens.
361.57 Athletic Greens is an all-in-one
363.24 vitamin mineral probiotic drink.
365.68 I started taking Athletic
Greens way back in 2012.
369.05 And so I'm delighted that
they're sponsoring the podcast.
371.91 The reason I started
taking Athletic Greens
373.76 and the reason I still
take Athletic Greens
375.84 is that it covers all of my
vitamin mineral probiotic basis.
379.47 In fact, when people ask
me, what should I take?
382.05 I always suggest that the
first supplement people take
384.49 is Athletic Greens,
385.65 for the simple reason,
386.96 is that the things that
contains covers your bases
389.52 for metabolic health, endocrine health,
391.92 and all sorts of other
systems in the body.
394.03 And the inclusion of probiotics
395.73 are essential for a
healthy gut microbiome.
398.617 There are now tons of data showing
400.57 that we have neurons in our gut,
402.85 and keeping those neurons healthy requires
404.8 that they are exposed
405.66 to what are called the correct microbiota,
407.98 little microorganisms that live in our gut
409.93 and keep us healthy.
411.15 And those neurons in turn
help keep our brain healthy.
413.9 They influence things like
mood, our ability to focus,
416.55 and many, many other
factors related to health.
419.93 With Athletic Greens, it's terrific,
421.44 because it also tastes really good.
423.2 I drink it once or twice a day.
424.74 I mix mine with water and
I add a little lemon juice,
427.08 or sometimes a little bit of lime juice.
429.43 If you want to try athletic greens,
431.13 you can go to athleticgreens.com/huberman.
434.13 And if you do that, you can
claim their special offer.
436.71 They're giving away
five free travel packs,
438.7 little packs that make it
easy to mix up Athletic Greens
441.08 while you're on the road.
442.228 And they'll give you a year supply
444.28 of vitamin D3 and K2.
446.31 Again, go to athleticgreens.com/huberman
448.97 to claim that special offer.
450.93 And now, my conversation
with Dr. Lex Fridman.
454.64 - We meet again.
455.58 - We meet again.
457.46 Thanks so much for sitting down with me.
459.65 I have a question that I think
is on a lot of people's minds
463.001 or ought to be on a lot of people's minds,
466.44 because we hear these
terms a lot these days,
470.09 but I think most people,
471.36 including most scientists and including me
475.03 don't know really what is
artificial intelligence,
480.09 and how is it different
481.377 from things like machine
learning and robotics?
485.29 So, if you would be so
kind as to explain to us,
488.96 what is artificial intelligence,
491.61 and what is machine learning?
494.04 - Well, I think that
question is as complicated
497.14 and as fascinating as the
question of, what is intelligence?
501.81 So, I think of artificial intelligence,
505.65 first, as a big philosophical thing.
508.78 Pamela McCormick said
AI was the ancient wish
517.21 to forge the gods,
518.34 or was born as an ancient
wish to forge the gods.
521.76 So I think at the big philosophical level,
524.29 it's our longing to create
other intelligence systems.
528.34 Perhaps systems more powerful than us.
531.77 At the more narrow level,
534.21 I think it's also set of tools
537.04 that are computational mathematical tools
539.29 to automate different tasks.
541.16 And then also it's our attempt
to understand our own mind.
545.66 So, build systems that exhibit
some intelligent behavior
549.96 in order to understand
what is intelligence
552.412 in our own selves.
554.63 So all of those things are true.
556.21 Of course, what AI really
means as a community,
559.36 as a set of researchers and engineers,
561.54 it's a set of tools,
562.68 a set of computational techniques
565.33 that allow you to solve various problems.
568.638 There's a long history
that approaches the problem
573.05 from different perspectives.
574.25 What's always been throughout
one of the threads,
577.69 one of the communities goes under the flag
580.31 of machine learning,
581.25 which is emphasizing in the AI space,
585.688 the task of learning.
588.13 How do you make a machine
589.56 that knows very little in the beginning,
591.2 follow some kind of process
593.479 and learns to become better and
better at a particular task?
597.71 What's been most very effective
in the recent about 15 years
603.7 is a set of techniques
that fall under the flag
605.73 of deep learning that
utilize neural networks.
608.7 When your networks are these
fascinating things inspired
612.97 by the structure of the human brain,
616.32 very loosely, but they have a,
618.46 it's a network of these little
basic computational units
621.6 called neurons, artificial neurons.
624.048 And they have,
625.82 these architectures have
an input and output.
628.76 They know nothing in the beginning,
630.19 and their task with learning
something interesting.
633.25 What that's something interesting is,
635.27 usually involves a particular task.
638.26 There's a lot of ways to talk about this
641.2 and break this down.
642.033 Like one of them is how
much human supervision
645.92 is required to teach this thing.
648.37 So supervised learning is broad category,
651.89 is the neural network knows
nothing in the beginning
656.21 and then it's given a bunch
of examples in computer vision
660.42 that will be examples of cats,
dogs, cars, traffic signs,
664.057 and then you're given the image
666.15 and you're given the ground
truth of what's in that image.
669.59 And when you get a large
database of such image examples
673.07 where you know the truth,
675.302 the neural network is
able to learn by example,
678.66 that's called supervised learning.
681.114 There's a lot of fascinating
questions within that,
683.88 which is, how do you provide the truth?
686.07 When you given an image of a cat,
689.916 how do you provide to the computer
692.96 that this image contains a cat?
694.96 Do you just say the entire
image is a picture of a cat?
698.01 Do you do what's very commonly been done,
700.37 which is a bounding box,
701.57 you have a very crude box
around the cat's face saying,
705.43 this is a cat?
706.55 Do you do semantic segmentation?
708.77 Mind you, this is a 2D image of a cat.
711.05 So it's not,
713.0 the computer knows nothing
713.98 about our three-dimensional world,
715.7 is just looking at a set of pixels.
717.37 So, semantic segmentation
is drawing a nice,
721.18 very crisp outline around the
cat and saying, that's a cat.
724.96 That's really difficult
to provide that truth.
727.11 And one of the fundamental open questions
729.55 in computer vision is,
730.83 is that even a good
representation of the truth?
733.84 Now, there's another
contrasting set of ideas
738.6 that our attention they're overlapping is,
741.58 well, it's used to be called
unsupervised learning.
744.37 What's commonly now called
self-supervised learning.
747.19 Which is trying to get less and less
749.46 and less human supervision into the task.
754.01 So self-supervised learning is more,
758.966 it's been very successful in
the domain of language model,
762.04 natural English processing,
763.22 and now more and more as being successful
765.0 in computer vision task.
766.51 And the idea there is,
768.9 let the machine without
any ground-truth annotation
773.18 just look at pictures on the internet,
775.77 or look at texts on the internet
777.63 and try to learn something generalizable
782.36 about the ideas that are
at the core of language
785.84 or at the core of vision.
787.23 And based on that,
789.61 we humans at its best like
to call that common sense.
792.94 So with this,
793.773 we have this giant base of knowledge
796.03 on top of which we build
more sophisticated knowledge.
798.75 We have this kind of
commonsense knowledge.
801.57 And so the idea with
self-supervised learning
803.42 is to build this
commonsense knowledge about,
807.32 what are the fundamental visual ideas
810.47 that make up a cat and a dog
812.05 and all those kinds of things
813.31 without ever having human supervision?
815.81 The dream there is the,
818.471 you just let an AI system
that's self supervised
823.35 run around the internet for awhile,
824.495 watch YouTube videos for
millions and millions of hours,
827.5 and without any supervision be primed
831.42 and ready to actually learn
with very few examples
834.68 once the human is able to show up.
836.72 We think of children in this way,
839.3 human children,
840.18 is your parents only
give one or two examples
843.08 to teach a concept.
844.325 The dream with self-supervised learning
847.07 is that will be the same with machines.
850.12 That they would watch millions
of hours of YouTube videos,
854.01 and then come to a human
and be able to understand
856.82 when the human shows them, this is a cat.
859.4 Like, remember this' a cat.
860.83 They will understand that a cat
862.74 is not just the thing with pointy ears,
864.98 or a cat is a thing that's
orange, or is furry,
868.83 they'll see something more fundamental
870.85 that we humans might not actually be able
872.75 to introspect and understand.
873.94 Like, if I asked you,
875.09 what makes a cat versus a dog,
876.83 you wouldn't probably not
be able to answer that,
879.44 but if I showed you, brought
to you a cat and a dog,
882.79 you'll be able to tell the difference.
884.43 What are the ideas that your brain uses
887.09 to make that difference?
888.84 That's the whole dream with
self-supervised learning,
891.31 is it would be able to
learn that on its own.
893.93 That set of commonsense knowledge,
896.12 that's able to tell the difference.
897.91 And then there's like a
lot of incredible uses
901.64 of self-supervised learning,
904.04 very weirdly called self-play mechanism.
907.34 That's the mechanism behind
908.376 the reinforcement learning successes
912.41 of the systems that won at Go,
915.76 at, AlphaZero that won a chess.
918.89 - Oh, I see.
919.723 That play games?
920.87 - [Lex] That play games.
921.703 - Got it.
922.536 - So the idea of self-play is probably,
924.86 applies to other domains than just games.
927.92 Is a system that just
plays against itself.
930.9 And this is fascinating
in all kinds of domains,
933.62 but it knows nothing in the beginning.
936.64 And the whole idea is it creates
938.6 a bunch of mutations of itself
941.09 and plays against those
versions of itself.
946.67 And the fascinating thing is
when you play against systems
950.27 that are a little bit better than you,
951.85 you start to get better yourself.
953.75 Like learning,
954.583 that's how learning happens.
956.15 That's true for martial arts.
957.33 It's true in a lot of cases.
959.29 Where you want to be
interacting with systems
962.08 that are just a little better than you.
963.93 And then through this process
of interacting with systems
966.84 just a little better than you,
968.17 you start following this process
969.98 where everybody starts getting
better and better and better
971.98 and better until you are
several orders of magnitude
975.26 better than the world champion
in chess, for example.
977.947 And it's fascinating because
it's like a runaway system.
981.09 One of the most terrifying
and exciting things
983.59 that David Silver, the creator
of AlphaGo and AlphaZero,
987.24 one of the leaders of the team said,
990.38 to me is a,
991.98 they haven't found the
ceiling for AlphaZero.
996.67 Meaning it could just
arbitrarily keep improving.
999.4 Now, in the realm of chess,
1000.87 that doesn't matter to us.
1001.9 That it's like,
1003.03 it just ran away with the game of chess.
1005.02 Like it's like just so
much better than humans.
1007.935 But the question is what,
1009.47 if you can create that in the
realm that does have a bigger,
1014.33 deeper effect on human
beings and societies,
1017.91 that can be a terrifying process.
1020.0 To me, it's an exciting process
1021.82 if you supervise it correctly,
1023.75 if you inject, if what's
called value alignment,
1029.83 you make sure that the goals
that the AI is optimizing
1033.53 is aligned with human
beings and human societies.
1037.03 There's a lot of fascinating
things to talk about
1039.26 within the specifics of neural networks
1043.24 and all the problems that
people are working on.
1045.62 But I would say the really big,
1047.58 exciting one is self-supervised learning.
1049.66 We're trying to get less
and less human supervision,
1055.57 less and less human
supervision of neural networks.
1058.78 And also just a comment and I'll shut up.
1062.01 - No, please keep going.
1063.13 I'm learning.
1064.39 I have questions, but I'm learning.
1065.52 So please keep going.
1066.51 - So, to me what's
exciting is not the theory,
1069.47 it's always the application.
1070.818 One of the most exciting applications
1072.96 of artificial intelligence,
1075.14 specifically neural networks
and machine learning
1077.51 is Tesla Autopilot.
1079.17 So these are systems that are
working in the real world.
1081.62 This isn't an academic exercise.
1083.65 This is human lives at stake.
1085.25 This is safety-critical.
1087.25 - These are automated vehicles.
1088.577 Autonomous vehicles.
- Semi-autonomous.
1090.43 We want to be.
1091.3 - Okay.
1092.37 - We've gone through wars on these topics,
1095.0 - Semi-autonomous vehicles.
1096.31 - Semi-autonomous.
1097.143 So, even though it's called
a FSD, Full Self-Driving,
1102.59 it is currently not fully autonomous,
1105.0 meaning human supervision is required.
1107.78 So, human is tasked with
overseeing the systems.
1110.94 In fact, liability-wise, the
human is always responsible.
1115.25 This is a human factor
psychology question,
1117.99 which is fascinating.
1119.41 I'm fascinated by the whole space,
1123.03 which is a whole 'nother space
of human robot interaction
1126.2 when AI systems and humans work together
1128.8 to accomplish tasks.
1130.01 That dance to me is one of
the smaller communities,
1135.44 but I think it will be one of the most
1137.63 important open problems
once they're solved,
1140.37 is how the humans and
robots dance together.
1143.561 To me, semi-autonomous driving
is one of those spaces.
1147.573 So for Elon, for example,
he doesn't see it that way,
1151.72 he sees semi-autonomous
driving as a stepping stone
1156.55 towards fully autonomous driving.
1158.66 Like, humans and robots
can't dance well together.
1162.76 Like, humans and humans dance
and robots and robots dance.
1165.37 Like, we need to,
1166.461 this is an engineering problem,
1168.1 we need to design a perfect
robot that solves this problem.
1171.73 To me forever,
1172.78 maybe this is not the case with driving,
1174.17 but the world is going
to be full of problems
1177.17 with always humans and
robots have to interact,
1180.43 because I think robots
will always be flawed,
1183.55 just like humans are going
to be flawed, are flawed.
1187.169 And that's what makes life beautiful,
1190.04 that they're flawed.
1191.04 That's where learning happens
1192.4 at the edge of your capabilities.
1195.89 So you always have to figure out,
1197.56 how can flawed robots and
flawed humans interact together
1203.522 such that they, like the sum
is bigger than the whole,
1208.4 as opposed to focusing on just
building the perfect robot?
1211.58 - Mm-hmm.
1212.59 - So that's one of the
most exciting applications
1215.37 I would say of artificial
intelligence to me
1217.83 is autonomous driving, the
semi-autonomous driving.
1220.69 And that's a really good
example of machine learning
1223.29 because those systems
are constantly learning.
1226.8 And there's a process there
that maybe I can comment on,
1231.28 the, Andrej Karpathy who's
the head of autopilot
1233.957 calls it the data engine.
1236.098 And this process applies for
a lot of machine learning,
1238.94 which is you build a
system that's pretty good
1240.94 at doing stuff,
1241.851 you send it out into the real world,
1245.38 it starts doing the stuff
1246.87 and then it runs into what
are called edge cases,
1249.23 like failure cases,
1250.7 where it screws up.
1252.491 We do this as kids.
1253.99 That you have-
1254.997 - You do this as adults.
1256.38 - We do this as adults.
1257.358 Exactly.
1258.62 But we learn really quickly.
1260.17 But the whole point,
1261.38 and this is the fascinating
thing about driving,
1263.309 is you realize there's
millions of edge cases.
1267.0 There's just like weird situations
that you did not expect.
1271.02 And so the data engine process
1273.12 is you collect those edge cases,
1275.15 and then you go back to the drawing board
1277.11 and learn from them.
1278.047 And so you have to
create this data pipeline
1281.02 where all these cars,
1282.69 hundreds of thousands of
cars are driving around
1285.38 and something weird happens.
1287.08 And so whenever this weird detector fires,
1290.94 it's another important concept,
1293.43 that piece of data goes
back to the mothership
1297.43 for the training,
1299.36 for the retraining of the system.
1300.88 And through this data engine process,
1302.68 it keeps improving and
getting better and better
1304.423 and better and better.
1305.91 So basically you send out
1307.65 a pretty clever AI
systems out into the world
1310.56 and let it find the edge cases,
1314.46 let it screw up just enough to figure out
1317.24 where the edge cases are,
1318.57 and then go back and learn from them,
1320.84 and then send out that new version
1322.67 and keep updating that version.
1324.28 - Is the updating done by humans?
1326.34 - The annotation is done by humans.
1328.98 The, so you have to,
1331.07 the weird examples come back,
1333.47 the edge cases,
1334.83 and you have to label what
actually happened in there.
1337.73 There's also some mechanisms
for automatically labeling,
1343.18 but mostly,
1344.23 I think you always have to
rely on humans to improve,
1346.79 to understand what's
happening in the weird cases.
1350.04 And then there's a lot of debate.
1351.79 And this, the other thing,
1352.69 what is artificial intelligence?
1354.5 Which is a bunch of smart people
1356.85 having very different opinions
about what is intelligence.
1359.78 So AI is basically a community of people
1361.92 who don't agree on anything.
1363.178 - And it seems to be the case.
1365.97 First of all,
1366.93 this is a beautiful description of terms
1368.7 that I've heard many times
among my colleagues at Stanford,
1371.94 at meetings in the outside world.
1373.8 And there are so many fascinating things.
1375.99 I have so many questions,
1376.98 but I do want to ask one
question about the culture of AI,
1380.25 because it does seem to be a community
1381.76 where at least as an outsider,
1384.0 where it seems like there's
very little consensus
1386.18 about what the terms
1387.33 and the operational definitions even mean.
1389.39 And there seems to be a lot
of splitting happening now
1392.24 of not just supervised
and unsupervised learning,
1394.93 but these sort of intermediate conditions
1398.11 where machines are autonomous,
1400.82 but then go back for more instruction
1402.15 like kids go home from
college during the summer
1404.05 and get a little,
1405.26 moms still feeds them
1406.27 then eventually they leave
the nest kind of thing.
1409.8 Is there something in
particular about engineers,
1412.67 or about people in this
realm of engineering
1415.86 that you think lends
itself to disagreement?
1419.12 - Yeah, I think,
1421.27 so, first of all, the
more specific you get,
1423.48 the less disagreement there is.
1424.68 So there's lot of disagreement
1425.96 about what is artificial intelligence,
1427.9 but there's less disagreement
about what is machine learning
1430.67 and even less when you
talk about active learning
1432.73 or machine teaching or
self-supervised learning.
1436.63 And then when you get
1437.55 into like NLP language
models or transformers,
1440.67 when you get into specific
neural network architectures,
1443.82 there's less and less
and less disagreement
1445.67 about those terms.
1446.71 So you might be hearing the disagreement
1448.1 from the high-level terms,
1449.37 and that has to do with
the fact that engineering,
1452.11 especially when you're talking
about intelligence systems
1455.24 is a little bit of an art and a science.
1460.87 So the art part is the thing
that creates disagreements,
1465.36 because then you start
having disagreements
1468.66 about how easy or difficult
the particular problem is.
1473.91 For example,
1474.743 a lot of people disagree with Elon
1477.61 how difficult the problem
of autonomous driving is.
1481.14 And so, but nobody knows.
1483.16 So there's a lot of disagreement about,
1484.77 what are the limits of these techniques?
1487.2 And through that,
1488.49 the terminology also contains
within it the disagreements.
1494.0 But overall,
1494.833 I think it's also a young science
1496.81 that also has to do with that.
1498.86 So like it's not just engineering,
1501.23 it's that artificial intelligence truly
1503.7 is a large-scale discipline,
1505.52 where it's thousands, tens of thousands,
1508.14 hundreds of thousands
of people working on it,
1510.07 huge amounts of money being
made as a very recent thing.
1513.87 So we're trying to figure out those terms.
1516.49 And, of course,
1517.323 there's egos and personalities
and a lot of fame to be made.
1522.65 Like the term deep learning, for example,
1525.77 neural networks have been around
1527.02 for many, many decades since the '60s,
1528.657 you can argue since the '40s.
1530.88 So there was a rebranding
of neural networks
1533.49 into the word, deep learning,
1535.45 term, deep learning,
1536.604 that was part of the
re-invigoration of the field,
1540.94 but it's really the same exact thing.
1542.72 - I didn't know that.
1543.67 I mean, I grew up in
the age of neuroscience
1546.06 when neural networks were discussed,
1549.09 computational neuroscience
and theoretical neuroscience,
1551.42 they had their own journals.
1553.11 It wasn't actually
taken terribly seriously
1555.06 by experimentalists until a few years ago.
1557.26 I would say about five to seven years ago.
1560.72 Excellent theoretical
neuroscientist like Larry Abbott
1563.54 and other colleagues,
1566.35 certainly at Stanford as well
1567.363 that people started paying attention
1568.83 to computational methods.
1570.36 But these terms,
1571.193 neural networks, computational methods,
1573.25 I actually didn't know
that neural network works
1575.18 in deep learning where
those have now become
1578.57 kind of synonymous.
1579.461 - No, they're always the same thing.
1582.74 - Interesting.
1583.573 It was, so.
1584.406 - I'm a neuroscientist
and I didn't know that.
1585.79 - So, well, because neural networks
1587.73 probably means something else
1588.97 and neural science not something else,
1590.23 but a little different flavor
depending on the field.
1592.64 And that's fascinating too,
1594.12 because neuroscience and AI people
1597.1 have started working together
and dancing a lot more
1600.392 in the recent,
1601.6 I would say probably decade.
1603.06 - Oh, machines are going into the brain.
1605.992 I have a couple of questions,
1607.65 but one thing that I'm sort of fixated on
1609.84 that I find incredibly interesting
1611.88 is this example you gave of playing a game
1615.77 with a mutated version of
yourself as a competitor.
1619.46 - Yeah.
1620.293 - I find that incredibly interesting
1622.36 as a kind of a parallel or a mirror
1625.25 for what happens when we
try and learn as humans,
1627.64 which is we generate repetitions
1630.03 of whatever it is we're trying to learn,
1631.96 and we make errors.
1633.28 Occasionally we succeed.
1635.06 In a simple example, for instance,
1636.7 of trying to throw bulls
eyes on a dartboard.
1638.8 - Yeah.
1639.633 - I'm going to have
errors, errors, errors.
1640.51 I'll probably miss the dartboard.
1641.73 And maybe occasionally, hit a bullseye.
1643.44 And I don't know exactly
what I just did, right?
1645.946 But then let's say I was playing darts
1648.76 against a version of myself
1650.37 where I was wearing a visual prism,
1652.48 like my visual, I had a visual defect,
1656.65 you learn certain things
in that mode as well.
1658.98 You're saying that a machine
can sort of mutate itself,
1662.92 does the mutation always
cause a deficiency
1665.13 that it needs to overcome?
1666.56 Because of mutations in biology
1668.01 sometimes give us super powers, right?
1669.63 Occasionally, you'll get somebody
1671.1 who has better than 2020 vision,
1672.75 and they can see better than
99.9% of people out there.
1676.41 So, when you talk about
a machine playing a game
1679.17 against a mutated version of itself,
1681.37 is the mutation always say what
we call a negative mutation,
1684.266 or an adaptive or a maladaptive mutation?
1687.68 - No, you don't know until you get,
1691.01 so, you mutate first
1692.06 and then figure out and they
compete against each other.
1694.5 - So, you're evolving,
1695.83 the machine gets to evolve
itself in real time.
1698.43 - Yeah.
1699.263 And I think of it,
1700.86 which would be exciting
1701.86 if you could actually do with humans.
1703.64 It's not just.
1705.21 So, usually you freeze
a version of the system.
1710.37 So, really you take on Andrew of yesterday
1713.96 and you make 10 clones of them.
1716.64 And then maybe you mutate, maybe not.
1719.02 And then you do a bunch of competitions
1721.13 of the Andrew of today,
like you fight to the death,
1724.12 and who wins last.
1725.69 So, I love that idea
1726.61 of like creating a bunch
of clones of myself
1729.04 from like from each of
the day for the past year,
1732.43 and just seeing who's going to be better
1734.58 at like podcasting or science,
1736.55 or picking up chicks at
a bar or I don't know,
1740.23 or competing in Jujitsu.
1742.26 That's the one way to do it,
1743.17 I mean, a lot of Lexes would
have to die for that process,
1746.4 but that's essentially what happens,
1747.93 is in reinforcement learning
1749.48 through the self-play mechanisms,
1751.014 it's a graveyard of systems
that didn't do that well.
1754.37 And the surviving, the good ones survive.
1759.8 - Do you think that, I mean,
Darwin's Theory of Evolution
1763.74 might have worked in
some sense in this way,
1766.34 but at the population level.
1767.77 I mean, you get a bunch of birds
with different shaped beaks
1769.6 and some birds have the shaped beak
1771.01 that allows them to get the seeds.
1772.32 I mean, is a trivially simple example
1774.92 of Darwinian in evolution,
1776.22 but I think it's correct even
though it's not exhaustive.
1780.85 Is what you're referring to?
1782.19 You essentially that normally this is done
1784.13 between members of a different species,
1785.6 lots of different members of
species have different traits
1788.03 and some get selected for,
1789.005 but you could actually create
multiple versions of yourself
1792.62 with different traits.
1793.92 - So, with, I should
probably have said this,
1796.5 but perhaps it's implied
with machine learning,
1799.963 with reinforcement learning
through these processes.
1802.52 One of the big requirements,
1803.885 is to have an objective
function, a loss function,
1806.54 a utility function,
1807.84 those are all different
terms for the same thing,
1809.669 is there's a like any equation
that says what's good,
1815.12 and then you're trying to
optimize that equation.
1817.55 So, there's a clear
goal for these systems.
1820.602 - Because it's a game, like
with chess, there's a goal.
1823.96 - But for anything.
1825.32 Anything you want machine
learning to solve,
1827.69 there needs to be an objective function.
1829.97 In machine learning, it's
usually called Loss Function,
1832.307 that you're optimizing.
1834.38 The interesting thing about evolution,
1837.54 it's complicated of course,
1838.74 but the goal also seems to be evolving.
1841.74 Like it's a, I guess,
adaptation to the environment,
1844.05 is the goal,
1844.99 but it's unclear that you
can convert that always.
1848.32 It's like survival of the fittest.
1852.18 It's unclear what the fittest is.
1853.89 In machine learning,
1855.37 the starting point,
1856.85 and this is like what
human ingenuity provides,
1860.49 is that fitness function of
what's good and what's bad,
1864.45 which it lets you know which
of the systems is going to win.
1868.38 So, you need to have a equation like that.
1870.579 One of the fascinating
things about humans,
1872.89 is we figure out objective
functions for ourselves.
1877.17 Like it's the meaning of life,
1880.54 like why the hell are we here?
1882.99 And a machine currently
1885.11 has to have a hard-coded
statement about why.
1889.27 - It has to have a meaning of-
1890.53 - Yeah.
1891.363 - Artificial intelligence-based life.
1893.28 - Right.
1894.113 It can't.
1894.946 So, like there's a lot of
interesting explorations
1897.63 about that function being
more about curiosity,
1902.44 about learning new things
and all that kind of stuff,
1905.27 but it's still hard coded.
1906.72 If you want a machine to be
able to be good at stuff,
1909.62 it has to be given very clear statements
1913.48 of what good at stuff means.
1916.11 That's one of the challenges
of artificial intelligence,
1918.4 is you have to formalize the,
1921.63 in order to solve a problem,
you have to formalize it
1923.897 and you have to provide
1926.19 both like the full sensory information,
1928.29 you have to be very clear
about what is the data
1930.667 that's being collected,
1932.208 and you have to also be clear
about the objective function.
1935.9 What is the goal that
you're trying to reach?
1938.88 And that's a very difficult thing
1940.75 for artificial intelligence.
1942.14 - I love that you mentioned curiosity,
1943.99 I am sure this definition
falls short in many ways,
1946.98 but I define curiosity
1948.47 as a strong interest in knowing something,
1953.19 but without an attachment to the outcome.
1955.598 It's sort of a, it could
be a random search,
1959.36 but there's not really
an emotional attachment,
1962.1 it's really just a desire
1963.37 to discover and unveil what's there
1965.57 without hoping it's a
gold coin under a rock,
1968.91 you're just looking under rocks.
1970.269 Is that more or less how the,
1972.39 within machine learning,
1973.91 it sounds like there are elements
1975.32 of reward prediction and rewards.
1978.6 The machine has to know when
it's done the right thing.
1981.5 So, can you make machines
that are curious,
1985.32 or are the sorts of machines
that you are describing,
1988.05 curious by design?
1990.25 - Yeah, curiosity is a kind
of a symptom, not the goal.
1996.31 So, what happens,
1998.2 is one of the big trade-offs
in reinforcement learning,
2002.33 is this exploration versus exploitation.
2005.195 So, when you know very little,
it pays off to explore a lot,
2009.64 even suboptimal, like even trajectories
2012.47 that seem like they're not
going to lead anywhere,
2014.67 that's called exploration.
2016.21 The smarter and smarter
and smarter you get,
2018.247 the more emphasis you put on exploitation,
2021.86 meaning you take the best
solution, you take the best path.
2025.369 Now, through that process,
2027.25 the exploration can look
like curiosity by us humans,
2032.567 but it's really just trying to
get out of the local optimal,
2035.71 the thing it's already discovered.
2038.78 From an AI perspective,
2040.2 it's always looking to optimize
the objective function,
2044.41 it derives, and we can
talk about the slot more,
2048.19 but in terms of the tools
of machine learning today,
2051.55 it derives no pleasure
from just the curiosity
2055.99 of like, I don't know, discovery.
2059.522 - So, there's no dopamine
2060.605 for machine learning.
- There's no dopamine.
2061.86 - There's no reward, system chemical,
2064.03 or I guess electronic-reward system.
2066.93 - That said, if you look at
machine learning literature
2070.41 and reinforcement learning literature,
2072.1 that will use,
2073.01 like deep mind, we use
terms like dopamine,
2075.78 we're constantly trying
to use the human brain
2078.86 to inspire totally new
solutions to these problems.
2081.87 So, they'll think like,
2082.75 how does dopamine function
in the human brain,
2085.0 and how can it lead to
more interesting ways
2089.16 to discover optimal solutions?
2091.52 But ultimately currently,
2094.14 there has to be a formal
objective function.
2097.5 Now, you could argue
2098.333 the humans also has a set
of objective functions
2100.51 we try and optimize, we're just
not able to introspect them.
2104.55 - Yeah, we don't actually
know what we're looking for
2107.93 and seeking and doing.
2109.35 - Well, like Lisa Feldman Barrett
2111.05 who we spoken with at least
on Instagram, I hope you-
2113.56 - I met her through you, yeah.
2114.81 - Yeah, I hope you actually
have are on this podcast.
2117.58 - Yes, she's terrific.
2118.85 - So, she has a very,
2122.482 it has to do with homeostasis like that.
2126.82 Basically, there's a very
dumb objective function
2128.743 that the brain is trying to optimize,
2130.89 like to keep like body
temperature the same.
2132.97 Like there's a very dom
2134.35 kind of optimization function happening.
2136.48 And then what we humans do
with our fancy consciousness
2139.58 and cognitive abilities, is
we tell stories to ourselves
2142.045 so we can have nice podcasts,
2144.08 but really it's the brain
trying to maintain a,
2148.11 just like healthy state, I guess.
2150.76 That's fascinating.
2151.91 I also see the human brain,
2155.89 and I hope artificial
intelligence systems,
2158.99 as not just systems that solve
problems, or optimize a goal,
2164.21 but are also storytellers.
2166.3 I think there's a power
to telling stories.
2168.86 We tell stories to each other,
that's what communication is.
2171.374 Like when you're alone, that's
when you solve problems,
2176.72 that's when it makes sense to
talk about solving problems.
2179.09 But when you're a community,
2180.86 the capability to
communicate, tell stories,
2185.47 share ideas in such a way
that those ideas are stable
2188.24 over a long period of time,
2190.03 that's like, that's being
a charismatic storyteller.
2193.34 And I think both humans
are very good at this.
2195.9 Arguably, I would argue
that's why we are who we are,
2200.26 is we're great storytellers.
2201.993 And then AI I hope will also become that.
2204.82 So, it's not just about
being able to solve problems
2207.48 with a clear objective function,
2209.06 it's afterwards, be able
to tell like a way better,
2211.95 like make up a way better story
about why you did something,
2214.82 or why you failed.
2215.8 - So, you think that robots or,
and/or machines of some sort
2219.88 are going to start telling human stories?
2222.37 - Well, definitely.
2223.48 So, the technical field for
that is called Explainable AI,
2227.38 Explainable Artificial Intelligence,
2229.35 is trying to figure out
how you get the AI system
2233.877 to explain to us humans
why the hell it failed,
2237.56 or why it succeeded,
2239.075 or there's a lot of different
sort of versions of this,
2242.36 or to visualize how it
understands the world.
2246.35 That's a really difficult problem,
2248.14 especially with neural networks
2249.67 that are famously opaque,
2252.824 that we don't understand in many cases,
2256.36 why a particular neural network
does what it does so well,
2260.51 and to try to figure out
where it's going to fail,
2263.73 that requires the AI to explain itself.
2266.28 There's a huge amount of money,
2269.57 like there's a huge
amount of money in this,
2272.39 especially from government
funding and so on.
2274.6 Because if you want to deploy
AI systems in the real world,
2279.149 we humans at least, want
to ask it a question like,
2282.9 why the hell did you do that?
2284.3 Like in a dark way,
2286.7 why did you just kill that person, right?
2289.17 Like if a car ran over a person,
2290.66 we want to understand why that happened.
2293.05 And now again, we're sometimes
very unfair to AI systems
2297.93 because we humans can often
not explain why very well.
2301.92 But that's the field of Explainable AI
2305.6 that people are very interested in
2308.21 because the more and more
we rely on AI systems,
2311.54 like the Twitter recommender system,
2314.06 that AI algorithm that's, I
would say impacting elections,
2319.19 perhaps starting wars, or
at least military conflict,
2322.02 that algorithm, we want
to ask that algorithm,
2326.05 first of all, do you know
what the hell you're doing?
2328.53 Do you understand the society-level
effects you're having?
2332.77 And can you explain the
possible other trajectories?
2335.86 Like we would have that kind
of conversation with a human,
2338.35 we want to be able to do that with an AI.
2340.07 And in my own personal level,
2342.06 I think it would be nice
to talk to AI systems
2345.48 for stupid stuff, like
robots when they fail to-
2351.65 - Why'd you fall down the stairs?
2352.94 - Yeah.
2353.773 But not an engineering question,
2355.91 but almost like an endearing question,
2358.842 like I'm looking for,
2361.65 if I fell and you and I were hanging out,
2365.04 I don't think you need an explanation
2368.07 exactly what were the dynamics,
2369.89 like what was the under
actuated system problem here?
2372.81 Like what was the texture of the floor?
2375.65 Or so on.
2376.483 Or like, what was the-
2377.427 - No, I want to know what you're thinking.
2379.06 - That, or you might joke about like,
2381.25 you're drunk again, go home, or something,
2383.14 like there could be humor in
it, that's an opportunity.
2386.99 Like storytelling, isn't just
explanation of what happened,
2390.662 it's something that makes people laugh,
2394.44 it makes people fall in
love, it makes people dream,
2397.51 and understand things
2399.01 in a way that poetry makes
people understand things
2402.02 as opposed to a rigorous log
2404.68 of where every sensor was,
where every actuator was.
2409.59 - I mean, I find this incredible
2411.51 because one of the hallmarks
2413.91 of severe autism spectrum disorders is,
2417.67 a report of experience
from the autistic person
2421.9 that is very much a
catalog of action steps.
2425.35 It's like, how do you feel today?
2426.42 And they'll say, well,
I got up and I did this,
2428.01 and then I did this, and I did this.
2429.13 And it's not at all the way that a person
2431.72 who doesn't have autism
spectrum disorder would respond.
2435.75 And the way you describe these machines
2438.8 has so much humanism,
2442.45 or so much of a human
and biological element,
2445.46 but I realized that we were
talking about machines.
2448.926 I want to make sure that I understand
2451.72 if there's a distinction
between a machine that learns,
2457.9 a machine with artificial
intelligence and a robot.
2460.84 Like at what point does
a machine become a robot?
2463.89 So, if I have a ballpoint pen,
2466.5 I'm assuming I wouldn't call that a robot,
2468.7 but if my ballpoint pen can come to me
2472.44 when I moved to the
opposite side of the table,
2475.36 if it moves by whatever mechanism,
2477.97 at that point, does it become a robot?
2480.7 - Okay, there's 1 million
ways to explore this question.
2483.46 It's a fascinating one.
2485.13 So, first of all, there's
a question of what is life?
2489.26 Like how do you know something
as a living form and not?
2492.5 And it's to the question
of when does sort of a,
2495.5 maybe a cold computational
system becomes a,
2500.237 or already loading these
words with a lot of meaning,
2502.72 robot and machine,
2506.24 So, one, I think movement is important,
2510.69 but that's a kind of a boring idea
2512.37 that a robot is just a machine
2514.227 that's able to act in the world.
2516.65 So, one artificial intelligence
2518.86 could be both just the thinking thing,
2521.84 which I think is what machine learning is,
2524.08 and also the acting thing,
2525.76 which is what we usually
think about robots.
2527.93 So, robots are the things
that have a perception system
2529.757 that's able to take in the world
2531.64 however you define the world,
2533.19 is able to think and learn
2534.64 and do whatever the hell it does inside,
2536.75 and then act on the world.
2538.73 So, that's the difference
2539.563 between maybe an AI system
learning machine and a robot,
2543.26 it's something that's able,
2544.3 a robot is something that's
able to perceive the world
2547.26 and act in the world.
2548.21 - So, it could be through
language or sound,
2551.12 or it could be through movement or both.
2552.69 - Yeah.
2553.734 And I think it could also
be in the digital space
2556.12 as long as there's a aspect of entity
2559.07 that's inside the machine
2560.954 and a world that's outside the machine.
2564.24 And there's a sense in which the machine
2566.48 is sensing that world and acting in it.
2569.37 - So, we could,
2570.203 for instance, there could
be a version of a robot,
2572.64 according to the definition
that I think you're providing,
2575.44 where the robot, where
I go to sleep at night
2578.28 and this robot goes and
forges for information
2581.47 that it thinks I want to
see loaded onto my desktop
2584.83 in the morning.
2585.663 There was no movement of that machine,
2587.04 there was no language,
2587.873 but it essentially, has
movement in cyberspace.
2591.17 - Yeah, there's a distinction
that I think is important
2598.43 in that there's an element
of it being an entity,
2604.21 whether it's in the digital
or the physical space.
2606.66 So, when you have something
like Alexa in your home,
2612.31 most of the speech recognition,
most of what Alexa is doing,
2616.76 is constantly being sent
back to the mothership.
2622.09 When Alexa is there on its
own, that's to me, a robot,
2626.99 when it's there
interacting with the world.
2629.094 When it's simply a finger
of the main mothership,
2634.374 then the Alexa is not a robot,
2636.7 then it's just an interaction device,
2638.91 then may be the main Amazon Alexa AI,
2642.4 big, big system is the robot.
2644.83 So, that's important because
there's some element,
2648.89 to us humans, I think, where
we want there to be an entity,
2652.6 whether in the digital
or the physical space,
2654.78 that's where ideas of
consciousness come in
2656.78 and all those kinds of things
2658.61 that we project our understanding
2660.753 of what it means to be a being.
2664.3 And so, to take that further,
2667.05 when does a machine become a robot,
2671.4 I think there's a special moment.
2675.26 There's a special moment
in a person's life
2677.757 and in a robot's life
where it surprises you.
2681.64 I think surprise is a
really powerful thing,
2684.31 where you know how the thing
works and yet it surprises you,
2690.03 that's a magical moment for us humans.
2692.01 So, whether it's a chess-playing program
2694.68 that does something that
you haven't seen before,
2697.65 that makes people smile like, huh,
2701.2 those moments happen with
AlphaZero for the first time
2704.35 in chess playing,
2705.6 where grand masters were
really surprised by a move.
2708.79 They didn't understand the move
2710.29 and then they studied and studied
2711.68 and then they understood it.
2713.41 But that moment of surprise,
2715.32 that's for grandmasters in chess.
2717.088 I find that moment of
surprise really powerful,
2720.45 really magical in just everyday life.
2723.36 - Because it supersedes the
human brain in that moment?
2728.14 - So, it's not supersedes,
like outperforms,
2731.04 but surprises you in a positive sense.
2735.42 Like I didn't think he could do that,
2738.27 I didn't think that you had that in you.
2740.77 And I think that moment is
a big transition for a robot
2745.21 from a moment of being a servant
2748.313 that accomplishes a particular task
2751.2 with some level of accuracy,
with some rate of failure,
2755.75 to an entity, a being that's struggling
2759.62 just like you are in this world.
2761.94 And that's a really important moment
2764.49 that I think you're not
going to find many people
2767.55 in the AI community that
talk like I just did.
2771.119 I'm not speaking like some
philosopher or some hippie,
2774.41 I'm speaking from purely
engineering perspective.
2776.82 I think it's really important
for robots to become entities
2780.125 and explore that as a
real engineering problem,
2783.39 as opposed to everybody treats robots
2785.92 in the robotics community,
2787.71 they don't even call them or he or she,
2789.58 they don't give them, try
to avoid giving them names,
2791.954 they've really want to see
like a system, like a servant.
2796.74 They see it as a servant that's
trying to accomplish a task.
2800.32 To me, and don't think I'm
just romanticizing the notion,
2804.7 I think it's a being, it's a
currently perhaps a dumb being,
2808.594 but in the long arc of history,
2813.1 humans are pretty dumb beings too, so-
2815.22 - I would agree with that statement.
2816.233 [Andrew laughing]
2817.14 - So, I tend to really want to
explore this treating robots
2821.55 really as entities, yeah.
2825.76 So, like anthropomorphization,
2828.44 which is the sort of the act
2829.95 of looking at a inanimate object
2832.53 and projecting onto it life-like features,
2835.49 I think robotics generally
sees that as a negative,
2841.26 I see it as a superpower.
2843.89 Like that, we need to use that.
2846.77 - Well, I'm struck
2847.75 by how that really grabs
onto the relationship
2850.61 between human and machine,
or human and robot.
2854.11 So, I guess the simple question is,
2856.71 and I think you've already
told us the answer,
2858.92 but does interacting
with a robot change you?
2864.01 In other words, do we develop
relationships to robots?
2868.07 - Yeah, I definitely think so.
2870.28 I think the moment you
see a robot or AI systems
2875.225 as more than just servants but entities,
2880.02 they begin to change you, in
just like good friends do,
2882.62 just like relationships
just to other humans.
2886.437 I think for that,
2888.33 you have to have certain
aspects of that interaction.
2891.49 Like the robot's ability to say no,
2895.98 to have its own sense of identity,
2899.13 to have its own set of goals,
2901.77 that's not constantly serving you,
2902.603 but instead, trying to
understand the world
2904.95 and do that dance of understanding
2907.25 through communication with you.
2908.96 So, I definitely think there's a,
2911.82 I mean, I have a lot of thoughts
about this as you may know,
2914.573 and that's at the core of
my life-long dream actually
2919.15 of what I want to do,
2919.983 which is I believe that most people
2924.41 have a notion of loneliness in them
2928.87 that we haven't discovered,
2930.34 that we haven't explored, I should say.
2932.742 And I see AI systems as
helping us explore that
2937.89 so that we can become better humans,
2940.28 better people towards each other.
2942.31 So, I think that connection
2945.13 between human and AI, human and robot,
2949.195 is not only possible, but will
help us understand ourselves
2954.56 in ways that are like
several orders of magnitude
2958.8 deeper than we ever could have imagined.
2961.36 I tend to believe that [sighing]
2964.38 well, I have very wild levels of belief
2972.53 in terms of how impactful
that will be, right?
2975.64 - So, when I think about
human relationships,
2977.78 I don't always break
them down into variables,
2981.19 but we could explore a
few of those variables
2983.45 and see how they map to
human-robot relationships.
2987.39 One is just time, right?
2989.19 If you spend zero time
with another person at all
2992.879 in cyberspace or on
the phone or in person,
2995.42 you essentially have no
relationship to them.
2997.88 If you spend a lot of time,
you have a relationship,
2999.71 this is obvious.
3000.543 But I guess one variable would be time,
3001.84 how much time you spend
with the other entity,
3005.22 robot or human.
3006.58 The other would be wins and successes.
3010.05 You enjoy successes together.
3013.87 I'll give a absolutely trivial
example this in a moment,
3016.72 but the other would be failures.
3019.21 When you struggle with somebody,
3021.56 whether or not you struggle
between one another,
3023.61 you disagree,
3024.55 like I was really struck by the fact
3025.7 that you said that robot saying no,
3027.1 I've never thought about
a robot saying no to me,
3030.13 but there it is.
3031.15 - I look forward to you
being one of the first people
3034.07 I send this robots to.
3035.33 - So do I.
3036.35 So, there's struggle.
3037.856 When you struggle with
somebody, you grow closer.
3041.21 Sometimes the struggles
3042.59 are imposed between those two people,
3044.7 so called trauma bonding,
3045.95 they call it in the whole
psychology literature
3048.35 and pop psychology literature.
3050.17 But in any case, I can imagine.
3052.25 So, time successes
together, struggle together,
3057.64 and then just peaceful time,
3060.06 hanging out at home, watching movies,
3063.01 waking up near one another,
3066.21 here, we're breaking down
3067.21 the elements of relationships of any kind.
3070.7 So, do you think that these elements
3073.44 apply to robot-human relationships?
3076.45 And if so, then I could see how,
3081.18 if the robot has its own entity
3084.11 and has some autonomy in
terms of how it reacts you,
3087.2 it's not just there just to serve you,
3088.99 it's not just a servant,
3090.23 it actually has opinions,
3091.94 and can tell you when maybe
your thinking is flawed,
3094.48 or your actions are flawed.
3095.82 - It can also leave.
3097.43 - It could also leave.
3099.41 So, I've never conceptualized
3100.95 robot-human interactions this way.
3103.95 So, tell me more about
how this might look.
3106.54 Are we thinking about a
human-appearing robot?
3111.41 I know you and I have both had
intense relationships to our,
3114.45 we have separate dogs obviously,
3116.21 but to animals,
3117.35 it sounds a lot like
human-animal interaction.
3119.17 So, what is the ideal
human-robot relationship?
3124.51 - So, there's a lot to be said here,
3126.4 but you actually pinpointed one
of the big, big first steps,
3130.71 which is this idea of time.
3133.79 And it's a huge limitation
3135.54 in machine-learning community currently.
3138.56 Now we're back to like the actual details.
3141.5 Life-long learning is a problem space
3146.36 that focuses on how AI systems
3149.18 can learn over a long period of time.
3151.97 What's currently most
machine learning systems
3154.932 are not able to do,
3157.43 is to all of the things
you've listed under time,
3159.82 the successes, the failures,
3161.87 or just chilling together watching movies,
3164.64 AI systems are not able to do that,
3168.0 which is all the
beautiful, magical moments
3171.08 that I believe are the days filled with,
3173.92 they're not able to keep track
of those together with you.
3177.372 - 'Cause they can't move
with you and be with you.
3179.25 - No, no, like literally we
don't have the techniques
3181.966 to do the learning,
3183.49 the actual learning of
containing those moments.
3187.04 Current machine-learning systems
3188.634 are really focused on
understanding the world
3191.89 in the following way,
3192.723 it's more like the perception system,
3194.38 like looking around, understand
like what's in the scene.
3200.03 That there's a bunch
of people sitting down,
3202.28 that there is cameras and microphones,
3204.593 that there's a table, understand that.
3207.147 But the fact that we shared
this moment of talking today,
3211.0 and still remember that
3212.82 for like next time you're doing something,
3216.69 remember that this moment happened.
3218.4 We don't know how to
do that technique-wise.
3220.44 This is what I'm hoping to innovate on
3224.18 as I think it's a very,
very important component
3227.13 of what it means to create
a deeper relationship,
3229.99 that sharing of moments together.
3232.09 - Could you post a photo
of you in the robot,
3234.11 like selfie with robot
3235.88 and the robot sees that image
3238.35 and recognizes that was time spent,
3241.063 there were smiles, or there were tears-
3243.42 - Yeah.
3244.253 - And create some sort of
metric of emotional depth
3249.08 in the relationship and
update its behavior?
3251.69 - So.
3253.41 - Could it...
3254.512 It texts you in the middle
of the night and say,
3255.345 why haven't you texted me back?
3256.84 - Well, yes, all of those
things, but we can dig into that.
3261.46 But I think that time element,
forget everything else,
3264.86 just sharing moments together,
that changes everything.
3269.25 I believe that changes everything.
3270.673 Now, there's specific things
3272.24 that are more in terms of
systems that I can explain you.
3277.35 It's more technical and
probably a little bit offline,
3279.42 'cause I have kind of wild ideas
3281.34 how that can revolutionize social networks
3284.79 and operating systems.
3287.268 But the point is that element alone,
3290.67 forget all the other
things we're talking about
3293.23 like emotions, saying no, all that,
3296.09 just remembering sharing moments together
3298.64 would change everything.
3300.3 We don't currently have systems
that share moments together.
3305.64 Like even just you and your fridge,
3308.13 just all those times,
you went late at night
3310.712 and ate thing you shouldn't have eaten,
3313.33 that was a secret moment you
have with your refrigerator.
3316.73 You shared that moment,
3318.0 that darkness or that beautiful moment
3320.4 where you were just like
heartbroken for some reason,
3324.1 you're eating that ice cream or whatever,
3326.35 that's a special moment.
3327.67 And that refrigerator was there for you,
3329.67 and the fact that it
missed the opportunity
3331.86 to remember that is tragic.
3336.04 And once it does remember that,
3338.8 I think you're going to be very
attached to the refrigerator.
3342.47 You're going to go through some
hell with that refrigerator.
3345.75 Most of us have, like
in the developed world,
3349.68 have weird relationships with food, right?
3351.56 So, you can go through some deep moments
3354.91 of trauma and triumph with food,
3357.3 and at the core of that,
is the refrigerator.
3359.23 So, a smart refrigerator, I
believe would change society.
3365.0 Not just the refrigerator,
3366.24 but these ideas in the
systems all around us.
3370.25 So, I just want to comment
3371.95 on how powerful that idea of time is.
3374.26 And then there's a bunch of elements
3375.85 of actual interaction of
allowing you as a human
3383.34 to feel like you're being heard.
3386.42 Truly heard, truly understood,
3389.15 that we human, like deep
friendship is like that, I think,
3393.08 but there's still an
element of selfishness,
3396.98 there's still an element
3397.93 of not really being able to
understand another human.
3400.76 And a lot of the times
3401.975 when you're going through trauma together,
3404.91 through difficult times
and through successes,
3407.63 you actually starting
3408.463 to get that inkling of
understanding of each other,
3411.25 but I think
3412.083 that could be done more
aggressively, more efficiently.
3417.46 Like if you think of a great therapist,
3419.31 I think I've never actually
been to a therapist,
3421.84 but I'm a believer I used to
want to be a psychiatrist.
3425.0 - Do Russians go to therapists?
3426.37 - No, they don't.
3427.34 They don't.
3428.173 And if they do, the therapist
don't live to tell the story.
3434.004 I do believe in talk therapy,
3436.05 which friendship is to
me, is it's talk therapy,
3438.62 or like you don't even necessarily
need to talk [laughing]
3443.74 it's like just connecting
through in the space of ideas
3446.927 and the space of experiences.
3448.93 And I think there's a lot of ideas
3450.81 of how to make AI systems
3452.21 to be able to ask the right questions
3454.569 and truly hear another human.
3457.36 This is what we try to do
with podcasting, right?
3460.59 I think there's ways to do that with AI.
3462.66 But above all else,
3464.48 just remembering the collection of moments
3468.49 that make up the day,
the week, the months,
3472.05 I think you maybe have
some of this as well.
3475.343 Some of my closest friends
3476.88 still are the friends from high school.
3479.76 That's time, we've been
through a bunch of together,
3483.01 and that like we're very different people.
3486.26 But just the fact that
we've been through that,
3487.697 and we remember those moments,
3489.35 and those moments somehow
create a depth of connection
3492.86 like nothing else, like
you and your refrigerator.
3497.13 - I love that because my graduate advisor,
3500.57 she unfortunately, she passed away,
3501.76 but when she passed away,
3502.63 somebody said at her at her memorial
3507.288 all these amazing things
she had done, et cetera.
3509.45 And then her kids got up there,
3511.79 and she had young children and that I knew
3513.45 as when she was pregnant with them.
3515.52 And so, it was really,
3516.76 you're even now I can feel
like your heart gets heavy,
3519.3 thinking about this,
3520.133 they're going to grow
up without their mother.
3521.86 And it was really amazing,
3522.693 very strong young girls,
and now the young women.
3526.954 And what they said was incredible,
3529.34 they said what they
really appreciated most
3531.77 about their mother, who
was an amazing person,
3535.85 is all the unstructured
time they spent together.
3539.31 - Mm-hmm.
3540.385 - So, it wasn't the trips to the zoo,
3541.218 it wasn't she woke up
at five in the morning
3543.65 and drove us to school.
3544.483 She did all those things too.
3545.55 She had two hour commute
in each direction,
3547.37 it was incredible, ran a lab, et cetera,
3549.4 but it was the unstructured time.
3551.49 So, on the passing of their mother,
3552.97 that's what they remembered
was that the biggest give
3556.56 and what bonded them to her,
3557.81 was all the time where
they just kind of hung out.
3560.45 And the way you describe the
relationship to a refrigerator
3564.06 is so, I want to say human-like,
3567.23 but I'm almost reluctant to say that.
3568.925 Because what I'm realizing
as we're talking,
3571.48 is that what we think of as human-like
3574.44 might actually be the
lower form of relationship.
3579.04 There may be relationships
that are far better
3582.4 than the sorts of relationships
3583.77 that we can conceive
in our minds right now
3586.87 based on what these machine
relationship interactions
3589.668 could teach us.
3591.14 Do I have that right?
3592.418 - Yeah, I think so.
3594.31 I think there's no reason to see machines
3595.77 as somehow incapable of
teaching us something
3599.93 that's deeply human.
3601.62 I don't think humans
have a monopoly on that.
3605.02 I think we understand
ourselves very poorly
3606.98 and we need to have the kind
of prompting from a machine.
3614.0 And definitely part of that,
3615.21 is just remembering the moments.
3618.96 I think the unstructured time together,
3624.23 I wonder if it's quite so unstructured.
3627.55 That's like calling this
podcast on structured time.
3630.57 - Maybe what they meant,
was it wasn't a big outing,
3633.91 there was no specific goal,
3636.21 but a goal was created
through the lack of a goal.
3639.68 Like we would just hang out
3640.59 and then you start playing, thumb war,
3642.55 and you end up playing
thumb war for an hour.
3644.63 So, it's the structure emerges
from lack of structure.
3648.95 - No, but the thing is the moments,
3652.34 there's something about those times
3654.36 that creates special moments,
3656.5 and I think those could be optimized for.
3660.33 I think we think of like a big outing as,
3662.1 I don't know, going to
Six Flags or something,
3663.85 or some big, the Grand Canyon,
3666.39 or go into some, I don't know,
I think we would need to,
3671.67 we don't quite yet understand, as humans,
3673.84 what creates magical moments.
3675.73 I think it's possible to
optimize a lot of those things.
3678.35 And perhaps like podcasting is
helping people discover that,
3681.41 like maybe the thing
we want to optimize for
3684.23 isn't necessarily like some
sexy, like quick clips,
3691.49 maybe what we want, is
long-form authenticity.
3694.54 - Depth.
3695.373 - Depth.
3696.29 So, we were trying to figure that out,
3698.264 certainly from a deep connection
3700.323 between humans and humans and NAS systems,
3704.18 I think long conversations, or
long periods of communication
3708.181 over a series of moments like my new,
3713.93 perhaps, seemingly
insignificant to the big ones,
3717.21 the big successes, the big failures,
3720.78 those are all just
stitching those together
3723.06 and talking throughout.
3725.31 I think that's the formula
3726.65 for a really, really deep connection.
3728.28 That from like a very specific
engineering perspective,
3731.97 is I think a fascinating open problem
3735.61 that has been really worked on very much.
3738.45 And for me, from a, if I have the guts
3741.88 and, I mean there's a
lot of things to say,
3744.84 but one of it is guts, is I'll
build a startup around it.
3749.53 - So, let's talk about this startup
3752.4 and let's talk about the dream.
3754.84 You mentioned this dream before
3756.08 in our previous conversations,
3757.32 always as little hints
dropped here and there.
3760.17 Just for anyone listening,
3761.42 there's never been an offline
conversation about this dream,
3763.75 I'm not privy to anything,
except what Lex says now.
3768.22 And I realized that there's no way
3769.8 to capture the full essence of a dream
3772.89 in any kind of verbal statement
3776.85 in a way that captures all of it.
3778.24 But what is this dream
3781.01 that you've referred to now several times
3783.89 when we've sat down together
and talked on the phone?
3786.97 Maybe it's this company,
maybe it's something distinct.
3789.13 If you feel comfortable,
3790.98 it'd be great if you
could share a little bit
3792.53 about what that is.
- Sure.
3793.62 So, the way people
express long-term vision,
3799.1 I've noticed is quite different.
3800.92 Like Elon is an example of somebody
3802.71 who can very crisply say
exactly what the goal is.
3807.062 Also has to do with the fact
that problems he's solving
3809.85 have nothing to do with humans.
3812.44 So, my long-term vision
3815.17 is a little bit more
difficult to express in words,
3819.02 I've noticed, as I've tried,
3820.9 it could be my brain's failure,
3822.51 but there's a way to sneak up to it.
3825.43 So, let me just say a few things.
3828.511 Early on in life and
also in the recent years,
3833.03 I've interacted with a few robots
3835.18 where I understood there's magic there.
3838.23 And that magic could be shared by millions
3842.16 if it's brought to light.
3844.69 When I first met Spot
from Boston Dynamics,
3847.673 I realized there's magic there
that nobody else is seeing.
3850.44 - Is the dog.
3851.47 - The dog, sorry.
3852.38 The Spot is the four-legged
robot from Boston Dynamics.
3857.28 Some people might have seen
it, it's this yellow dog.
3860.42 And sometimes in life,
3865.157 you just notice something
that just grabs you.
3868.47 And I believe that this is something
3870.81 that this magic is something that could be
3874.75 in every single device in the world.
3878.28 The way that I think maybe Steve Jobs
3881.42 thought about the personal computer,
3883.89 laws didn't think about the
personal computer this way,
3886.75 but Steve did.
3888.11 Which is like, he thought
that the personal computer
3890.37 should be as thin as a sheet of paper
3892.16 and everybody should have one,
3893.37 and this idea,
3894.808 I think it is heartbreaking
that we're getting,
3901.25 the world is being filled up with machines
3903.87 that are soulless.
3906.11 And I think every one of them
can have that same magic.
3910.002 One of the things that also inspired me
3914.83 in terms of a startup,
3916.09 is that magic can engineered
much easier than I thought.
3919.6 That's my intuition
3920.81 with everything I've
ever built and worked on.
3924.0 So, the dream is to
add a bit of that magic
3928.9 in every single computing
system in the world.
3932.4 So, the way that Windows
Operating System for a long time
3936.53 was the primary operating system
everybody interacted with,
3939.49 they built apps on top of it.
3940.982 I think this is something
that should be as a layer,
3945.513 it's almost as an operating system
3947.428 in every device that humans
interacted with in the world.
3951.04 Now, what that actually looks like,
3953.08 the actual dream when
I was officially a kid,
3957.97 it didn't have this
concrete form of a business,
3961.17 it had more of a dream of
exploring your own loneliness
3968.29 by interacting with machines, robots.
3973.38 This deep connection
between humans and robots
3975.75 was always a dream.
3977.21 And so, for me,
3978.99 I'd love to see a world where
there's every home as a robot,
3982.49 and not a robot that washes
the dishes, or a sex robot,
3987.89 or I don't know,
3990.02 think of any kind of
activity the robot can do,
3992.27 but more like a companion.
3993.88 - A family member.
3994.86 - A family member, the way a dog is.
3996.47 - Mm-hmm.
3997.8 - But a dog that's able to
speak your language too.
4002.01 So, not just connect the way
a dog does by looking at you
4006.02 and looking away
4006.853 and almost like smiling with
its soul in that kind of way,
4011.15 but also to actually
understand what the hell,
4014.62 like, why are you so
excited about the successes?
4016.78 Like understand the details,
understand the traumas.
4019.92 And that, I just think [sighing]
4022.48 that has always filled me
with excitement that I could,
4027.09 with artificial intelligence,
bring joy to a lot of people.
4032.41 More recently, I've been
more and more heartbroken
4038.07 to see the kind of division, derision,
4043.37 even hate that's boiling
up on the internet
4047.14 through social networks.
4048.68 And I thought this kind of
mechanism is exactly applicable
4052.8 in the context of social networks as well.
4054.83 So, it's an operating system
that serves as your guide
4061.558 on the internet.
4063.38 One of the biggest problems
4064.84 with YouTube and social
networks currently,
4067.99 is they're optimizing for engagement.
4070.8 I think if you create AI systems
4072.58 that know each individual person,
4076.26 you're able to optimize
for long-term growth
4079.14 for a long-term happiness.
4080.82 - Of the individual, or-
4081.653 - Of the individual, of the individual.
4083.76 And there's a lot of other things to say,
4086.4 which is in order for AI systems
4090.75 to learn everything about you,
4095.8 they need to collect,
4097.12 they need to just like you
and I when we talk offline,
4100.02 we're collecting data
4100.853 about each others
secrets about each other,
4103.56 the same way AI has to do that.
4106.66 And that allows you to,
4108.77 and that requires you to rethink
ideas of ownership of data.
4115.5 I think each individual
should own all of their data
4119.97 and very easily be able to leave
4121.95 just like AI systems can leave,
4123.61 humans can disappear
4126.667 and delete all of their
data in a moment's notice.
4130.67 Which is actually better
than we humans can do,
4133.97 is once we load the data
into each other, it's there.
4136.54 I think it's very important to be both,
4140.48 give people complete
control over their data
4143.634 in order to establish trust
that they can trust you.
4146.23 And the second part of
trust is transparency.
4149.5 Whenever the data is used
4150.98 to make it very clear
what is being used for.
4153.08 And not clear in a lawyerly legal sense,
4156.17 but clear in a way that people
4158.14 really understand what it's used for.
4159.65 I believe when people have the ability
4161.55 to delete all their data and walk away
4164.2 and know how the data is being
used, I think they'll stay.
4169.86 - The possibility of a clean breakup,
4171.584 is actually what will
keep people together.
4173.42 - Yeah, I think so.
4174.48 I think, exactly.
4176.43 I think a happy marriage
4178.69 acquires the ability to divorce easily
4182.109 without the divorce industrial
complex or whatever.
4186.72 Things currently going on
4187.553 and then there's so much money to be made
4189.44 from lawyers and divorce.
4190.77 But yeah, the ability to leave
4192.89 is what enables love, I think.
4195.444 - It's interesting.
4196.277 I've heard the phrase from
a semi-cynical friend,
4198.91 that marriage is the
leading cause of divorce,
4201.62 but now we've heard that divorce,
4203.292 or the possibility of divorce
4205.01 could be the leading cause of marriage.
4206.67 - Of a happy marriage.
4208.15 - Good point.
4208.983 - Of a happy marriage.
4209.96 So, yeah.
4210.793 So, there's a lot of details there,
4212.79 but the big dream
4213.97 is that connection between
AI system and a human.
4217.52 And I haven't.
4220.3 There's so much fear
4221.133 about artificial intelligence
systems and about robots
4223.91 that I haven't quite found the right words
4226.46 to express that vision
4227.62 because the vision I have,
4230.146 it's not like some
naive, delusional vision
4233.54 of like technology is
going to save everybody,
4236.47 it's I really do just have a positive view
4240.34 of ways AI systems can help
humans explore themselves.
4244.73 - I love that positivity
4246.08 and I agree that the stance everything
4249.64 is doomed is equally bad
4254.44 to say that everything's
going to turn out all right,
4256.6 there has to be a dedicated effort.
4258.33 And clearly, you're thinking
4260.42 about what that dedicated
effort would look like.
4262.55 You mentioned two aspects to
this dream [clears throat]
4266.63 and I want to make sure
4267.463 that I understand where
they connect if they do,
4270.07 or if they are independent streams.
4272.43 One was this hypothetical
robot family member,
4277.31 or some other form of robot
4279.39 that would allow people to
experience the kind of delight
4283.43 that you experienced many times,
4287.06 and that you would like the
world to be able to have,
4290.33 and it's such a beautiful
idea of this give.
4293.54 And the other is social media
4296.19 or social network platforms
that really serve individuals
4300.525 and their best selves and their
happiness and their growth.
4304.2 Is there crossover between those,
4305.64 or are these two parallel dreams?
4307.07 - It's 100% of the same thing.
4308.27 It's difficult to kind of explain
4310.93 without going through details,
4312.14 but maybe one easy way to explain
4314.67 the way I think about social networks,
4316.137 is to create an AI system that's yours.
4319.71 It's not like Amazon
Alexa that's centralized,
4323.26 you own the data,
4324.526 it's like your little friend
4327.117 that becomes your
representative on Twitter
4331.36 that helps you find things
that will make you feel good,
4336.25 that will also challenge your
thinking to make you grow,
4339.93 but not get to that,
4342.47 not let you get lost in the
negative spiral of dopamine,
4347.111 that gets you to be angry,
4349.47 or most just get you to
be not open to learning.
4354.28 And so, that little representative
4356.27 is optimizing your long-term health.
4360.41 And I believe that that is not
only good for human beings,
4365.64 it's also good for business.
4367.4 I think longterm, you
can make a lot of money
4370.52 by challenging this idea that
the only way to make money,
4374.55 is maximizing engagement.
4377.41 And one of the things that
people disagree with me on,
4379.73 is they think Twitter's
always going to win.
4382.45 Like maximizing engagement
is always going to win,
4384.92 I don't think so.
4386.34 I think people have woken up
now to understanding that,
4389.73 like they don't always feel good,
4392.667 the ones who are on Twitter a lot,
4396.27 that they don't always feel
good at the end of the week.
4399.33 - I would love feedback
4400.78 from whatever this creature, whatever,
4404.44 I don't know what to call it,
4406.77 as to maybe at the end of the week,
4408.76 it would automatically unfollow
4410.56 some of the people that I follow
4411.97 because it realized through
some really smart data
4415.64 about how I was feeling
inside, or how I was sleeping,
4418.09 or something that, I don't know,
4419.41 that just wasn't good for me.
4420.65 But it might also put things
and people in front of me
4423.64 that I ought to see,
4425.55 is that kind of a sliver
of what this look like?
4428.81 - The whole point, because
of the interaction,
4430.59 because of sharing the moments
and learning a lot about you,
4436.65 you're now able to
understand what interactions
4441.13 led you to become a better
version of yourself.
4444.32 Like the person you
yourself are happy with.
4446.849 This isn't,
4448.57 if you're into flat earth and
you feel very good about it,
4452.85 that you believe that earth is flat,
4455.76 like the idea that you should
sensor that is ridiculous.
4459.63 If it makes you feel good
4461.05 and you becoming the
best version of yourself,
4463.47 I think you should be getting
4464.65 as much flat earth as possible.
4466.47 Now, it's also good to
challenge your ideas,
4469.37 but not because the
centralized committee decided,
4474.407 but because you tell to the system
4477.32 that you like challenging your
ideas, I think all of us do.
4480.82 And then, which actually
YouTube doesn't do that well,
4484.09 once you go down the
flat-earth rabbit hole,
4485.96 that's all you're going to see.
4487.2 It's nice to get some really
powerful communicators
4491.69 to argue against flat earth.
4493.64 And it's nice to see that for you,
4497.05 and potentially, at least
long-term to expand your horizons,
4501.15 maybe the earth is not flat.
4503.29 But if you continue to
live your whole life
4505.21 thinking the earth is flat,
4506.7 I think, and you're being a
good father or son or daughter,
4511.72 and like you're being the
best version of yourself
4514.38 and you're happy with yourself,
I think the earth is flat.
4518.98 So, like I think this kind of idea,
4521.43 and I'm just using that kind
of silly, ridiculous example
4523.938 because I don't like the
idea of centralized forces
4530.45 controlling what you can and can't see,
4533.27 but I also don't like this idea
4534.98 of like not censoring anything.
4539.7 Because that's always the
biggest problem with that,
4542.58 is this there's a central decider,
4545.395 I think you yourself can decide
what you want to see and not,
4549.5 and it's good to have a
companion that reminds you
4554.07 that you felt last time you did this,
4555.838 or you felt good last time you did this.
4558.61 - Man, I feel like in every good story,
4560.15 there's a guide or a
companion that flies out,
4563.57 or forges a little bit further,
a little bit differently
4566.48 and brings back information that helps us,
4568.46 or at least tries to steer
us in the right direction.
4571.39 - So, yeah, that's
exactly what I'm thinking
4576.64 and what I've been working on.
4577.85 I should mention as a
bunch of difficulties here,
4580.332 you see me up and down
a little bit recently.
4584.67 So, there's technically
a lot of challenges here.
4588.21 Just like with a lot of technologies,
4590.367 and the reason I'm talking about it
4592.75 on a podcast comfortably as
opposed to working in secret,
4596.482 is it's really hard and
maybe it's time has not come.
4601.98 And that's something you have
to constantly struggle with
4604.17 in terms of like
entrepreneurially as a startup,
4608.11 like I've also mentioned
to you, maybe offline,
4610.68 I really don't care about money,
4612.223 I don't care about business success,
4615.33 all those kinds of things.
4618.54 So, it's a difficult decision to make,
4621.15 how much of your time do
you want to go all in here
4625.29 and give everything to this?
4628.053 It's a big roll the dice.
4629.8 Because I've also realized
4631.58 that's working on some of these problems,
4634.51 both with the robotics
and the technical side
4638.09 in terms of the machine-learning
system that I'm describing,
4643.16 it's lonely, it's really lonely.
4646.84 Because both on a personal
level and a technical level.
4651.46 So, on the technical level,
4652.53 I'm surrounded by people
that kind of doubt me,
4657.94 which I think all
entrepreneurs go through.
4660.56 And they doubt you in the following sense,
4662.84 they know how difficult it is.
4666.85 Like the people that
the colleagues of mine,
4669.57 they know how difficult
life-long learning is,
4672.31 they also know how difficult it is
4674.236 to build a system like this,
4675.807 to build the competitive social network.
4679.37 And in general,
4682.12 there's a kind of a loneliness
4685.06 to just working on something on your own
4688.83 for long periods of time.
4690.29 And you start to doubt whether,
4693.36 given that you don't have
a track record of success,
4696.35 like that's a big one.
4697.677 But when you look in the mirror,
4699.03 especially when you're young,
4700.55 but I still have that, I'm most things,
4702.71 you look in the mirror,
4703.73 it's like, and you have these big dreams,
4706.688 how do you know
4708.003 you're actually as smart
as you think you are?
4712.53 Like how do you know
4713.363 you're going to be able
to accomplish this dream?
4715.46 You have this ambition.
4716.57 - You sort of don't,
4717.72 but you're kind of pulling on a string
4720.95 hoping that there's a bigger ball of yarn.
4721.897 - Yeah.
4723.243 [Andrew laughing]
4724.076 But you have this kind of intuition.
4725.168 I think I pride myself in
knowing what I'm good at,
4731.066 because the reason I have that intuition,
4734.48 is 'cause I think I'm very good
4736.27 at knowing all the things I suck at,
4738.95 which is basically everything.
4740.943 So, like whenever I notice like,
4743.76 wait a minute, I'm kind of good at this,
4746.26 which is very rare for me.
4748.21 I think like that,
4749.1 that might be a ball of
yarn worth pulling at.
4751.55 And the thing with in terms
of engineering systems
4754.81 that are able to interact with humans,
4756.59 I think I'm very good at that.
4758.54 And because we talk about
podcasting and so on,
4762.07 I don't know if I'm very good at podcasts.
4763.59 - You're very good at podcasting, right?
4765.24 - But I certainly don't,
4767.22 I think maybe it is compelling for people
4770.81 to watch a kindhearted idiot
struggle with this form,
4775.4 maybe that's what's compelling.
4777.24 But in terms of like actual
being a good engineer
4780.63 of human-robot interaction
systems, I think I'm good.
4785.6 But it's hard to know until you do it,
4788.02 and then the world keeps
telling you you're not,
4791.28 and it's full of doll, it's really hard.
4793.85 And I've been struggling
with that recently,
4795.37 it's kind of a fascinating struggle.
4797.22 But then that's where the
Goggins thing comes in,
4799.82 is like, aside from the
state-hard motherfucker, is the,
4805.15 like whenever you're struggling,
4807.9 that's a good sign that if you keep going,
4811.21 that you're going to be
alone in the success, right?
4815.881 - Well, in your case, however, I agree.
4818.7 And actually David had a post recently
4820.38 that I thought was among
his many brilliant posts,
4823.26 was one of the more brilliant
4824.88 about how he talked about this myth
4827.38 of the light at the end of the tunnel.
4829.177 And instead, what he
replaced that myth with,
4833.054 was a concept that eventually,
your eyes adapt to the dark.
4838.057 That the tunnel, it's not
about a light at the end,
4840.35 that it's really about adapting
to the dark of the tunnel.
4842.553 It's very Goggins-
4843.523 - I love him so much.
4844.85 - Yeah.
4845.683 You got share a lot in common
knowing you both a bit,
4850.511 share a lot in common.
4852.55 But in this loneliness and
the pursuit of this dream,
4857.88 it seems to me,
4858.713 it has a certain component to
it that is extremely valuable,
4864.09 which is that the loneliness itself
4866.38 could serve as a driver
4868.04 to build the companion for the journey.
4870.51 - Well, I'm very deeply aware of that.
4873.71 So, like some people can make,
4877.009 'cause I talk about love a lot,
4878.67 I really love everything in this world,
4881.98 but I also love humans,
friendship and romantic,
4886.66 like even the cheesy stuff, just-
4891.07 - You like romantic movies?
4892.84 - Yeah, not those, not necessarily.
4894.42 So, well, I got so much
from Rogan about like,
4898.48 was it the Tango scene
from "Scent of a Woman,"
4901.13 but I find like there's nothing better
4904.04 than a woman in a red
dress, just like the classy-
4909.46 - You should move to Argentina my friend.
4910.6 My father's Argentine,
and you know what he said
4913.12 when I went on your
podcast for the first time?
4915.84 He said, he dresses well.
4918.38 Because in Argentina,
4919.22 the men go to a wedding
or a party or something,
4921.52 in the U.S., by halfway through the night,
4923.71 10 minutes in the night,
all the jackets are off.
4925.63 - Yeah.
4926.463 - It looks like everyone's
undressing for the party
4927.64 they just got dressed up for.
4928.94 And he said, you know, I
liked the way he dresses.
4931.64 And then when I started,
he was talking about you.
4933.23 And then when I started
my podcast, he said,
4935.86 why don't you wear a real
suit like your friend Lex?
4938.768 [laughing]
4940.569 - I remember that.
4941.402 - No, you can't.
4943.48 But let's talk about this
pursuit just a bit more,
4947.88 because I think what you're talking about,
4949.431 is building a, not just a
solution for loneliness,
4953.596 but you've alluded to
the loneliness as itself,
4956.81 an important thing.
4957.85 And I think you're right.
4958.683 I think within people,
4960.046 there is like caverns of faults and shame,
4965.13 but also just the desire
to have resonance,
4969.5 to be seen and heard.
4971.25 And I don't even know
4972.083 that it's seen and heard through language.
4974.98 But these reservoirs of loneliness,
4977.66 I think, well, they're interesting,
4981.27 maybe you could comment
a little bit about it.
4982.66 Because just as often
as you talk about love,
4985.2 I haven't quantified it,
4986.13 but it seems that you talk
about this loneliness,
4988.48 and maybe you just if you're willing,
4990.22 you'll share a little bit more about that,
4992.28 and what that feels like now
4994.66 in the pursuit of building
this robot-human relationship.
4998.157 And you've been, let me be direct,
5000.49 you've been spending a lot of time
5001.96 on building a robot-human
relationship, where's that at?
5008.41 - Oh, in terms of business,
in terms of systems?
5013.45 - No, I'm talking about a specific robot.
5015.56 - Oh [laughing]
5017.243 so, okay, I should mention a few things.
5019.99 So, one, is there's a startup with an idea
5022.7 where I hope millions of people can use.
5026.04 And then there's my own personal
5028.55 like almost like Frankenstein explorations
5031.646 with particular robots.
5035.09 So, I'm very fascinated with
the legged robots in my own,
5041.249 private sounds like dark,
5042.538 but like in of one experiments
5046.47 to see if I can recreate the magic.
5049.64 And that's been,
5051.36 I have a lot of really good
already perception systems
5055.195 and control systems that are
able to communicate affection
5059.28 in a dog-like fashion.
5060.81 So, I'm in a really good place there.
5062.55 The stumbling blocks,
5063.61 which also have been part
of my sadness recently,
5066.785 is that I also have to work
with robotics companies
5070.78 that I gave so much of my heart, soul
5074.557 and love and appreciation
towards Boston Dynamics,
5077.99 but Boston Dynamics has also,
5081.97 as a company, it has
to make a lot of money
5083.89 and they have marketing teams.
5085.6 And they're like looking
at this silly Russian kid
5088.13 in a suit and tie,
5089.37 it's like, what's he trying to do
5090.58 with all this love and robot interaction
5092.65 and dancing and so on?
5093.52 So, there was a, I
think let's say for now,
5099.05 it's like, when you break up
with a girlfriend or something,
5101.23 right now, we decided to part
ways on this particular thing.
5104.11 They're huge supporters of
mine, they're huge fans,
5105.94 but on this particular thing,
5108.7 Boston Dynamics is not focusing on,
5112.68 or interested in human-robot interaction.
5115.08 In fact, their whole business currently,
5117.21 is keep the robot as far
away from humans as possible
5120.465 because it's an in the industrial setting
5123.84 where it's doing monitoring
in dangerous environments.
5127.34 It's almost like a remote security camera,
5129.47 essentially is its application.
5132.07 To me, I thought it's still,
5134.6 even in those applications,
exceptionally useful
5137.15 for the robot to be able to
perceive humans, like see humans
5141.49 and to be able to, in a big map,
5145.55 localize where those humans
are and have human intention.
5148.44 For example, like this,
5149.84 I did this a lot of work with pedestrians,
5151.793 for a robot to be able to anticipate
5153.88 what the how the human is
doing, like where it's walking.
5157.481 The humans are not ballistics object,
5159.67 just because you're walking
this way one moment,
5161.64 it doesn't mean you'll keep
walking that direction,
5163.85 you have to infer a lot of signals,
5165.45 especially with the head
movement and the eye movement.
5167.427 And so, I thought that's
super interesting to explore,
5169.81 but they didn't feel that.
5171.73 So, I'll be working with a
few other robotics companies
5174.53 that are much more open
to that kind of stuff,
5178.39 and they're super
excited and fans of mine.
5180.7 And hopefully, Boston
Dynamics, my first love,
5182.962 like getting back with an
ex-girlfriend, will come around.
5186.04 But so, the algorithmically,
it's basically a done there.
5192.825 The rest, is actually getting
some of these companies
5196.74 to work with.
5197.573 And then there's,
5199.69 for people who'd worked with robots
5201.33 know that one thing is to
write software that works,
5204.93 and the other is to have a real
machine that actually works.
5208.46 And it breaks down all
kinds of different ways
5210.91 that are fascinating.
5211.75 And so, there's a big challenge there.
5213.98 But that's almost,
5217.05 it may sound a little bit confusing
5218.8 in the context of our previous discussion
5220.856 because the previous discussion
5223.36 was more about the big dream,
5224.62 how I hoped to have millions of people
5227.035 enjoy this moment of magic.
5229.24 This current discussion about a robot,
5232.19 is something I personally really enjoy,
5235.14 just brings me happiness,
5236.32 I really try to do now everything
that just brings me joy,
5240.79 I maximize that 'cause robots are awesome.
5244.23 But two, given my like
little bit growing platform,
5248.46 I want to use the opportunity
to educate people.
5252.83 Like robots are cool.
5254.14 And if I think they're cool,
5256.66 I hope be able to communicate
why they're cool to others.
5259.66 So, this little robot experiment
5262.19 is a little bit of research project too,
5264.02 there's a couple of publications
with MIT folks around that.
5267.05 But the other is just
to make some cool videos
5269.82 and explain to people
how they actually work.
5273.34 And as opposed to people
being scared of robots,
5276.373 they could still be
scared, but also excited,
5279.9 like see the dark side,
the beautiful side,
5283.17 the magic of what it means to bring,
5287.6 for a machine to become a robot.
5290.12 I want to inspire people with that.
5292.29 But that's less,
5293.89 it's interesting because
I think the big impact
5298.17 in terms of the dream does not
have to do with embodied AI.
5302.38 So, it does not need to have a body.
5304.06 I think the refrigerator's enough,
5306.82 that for an AI system just to
have a voice and to hear you,
5311.34 that's enough for loneliness.
5313.45 The embodiment is just-
5316.59 - By embodiment, you mean
the physical structure.
5318.48 - Physical instantiation of intelligence.
5321.29 So, it's a legged robot,
or even just a thing.
5326.04 I have a few other of humanoid robot,
5328.403 a little humanoid robot maybe
I'll keep them on the table,
5331.37 is like walks around,
5332.88 or even just like a mobile platform
5334.77 they can just like, turn
around and look at you,
5337.878 it's like we mentioned with the pen.
5339.23 Something that moves and can look at you,
5342.076 it's like that Butter Robot
that asks, what is my purpose?
5351.371 That is really, it's almost like art.
5355.68 There's something about a
physical entity that moves around,
5359.71 that's able to look at
you and interact with you,
5362.137 that makes you wonder
what it means to be human.
5365.97 It like challenges you to think,
5367.54 if that thing looks like
he has consciousness,
5372.58 what the hell am I?
5373.97 And I like that feeling,
5375.14 I think that's really useful for us,
5376.55 it's humbling for us humans.
5378.02 But that's less about research,
5381.13 it certainly less about business
5382.7 and more about exploring our own selves,
5386.01 and challenging others to think like,
5389.26 to think about what makes them human.
5392.93 - I love this desire
5394.81 to share the delight of an
interaction with a robot.
5398.07 And as you describe it,
5398.93 I actually find myself
starting to crave that
5401.37 because we all have those
elements from childhood where,
5404.96 or from adulthood, where
we experience something,
5406.81 we want other people to feel that.
5409.84 And I think that you're right,
5410.96 I think a lot of people are scared of AI,
5412.59 I think a lot of people
are scared of robots.
5414.49 My only experience, and
of a robotic-like thing
5418.498 is my Roomba Vacuum, where it goes about,
5421.89 actually, it was pretty good
at picking up Costello's hair
5424.3 when he was shed and
I was grateful for it.
5428.3 But then when I was on
a call or something,
5430.027 and it would get caught
on a wire or something,
5433.22 I would find myself getting
upset with the Roomba
5435.31 in that moment, I'm
like, what are you doing?
5437.07 And obviously it's just
doing what it does.
5439.92 But that's a kind of mostly positive,
5442.51 but slightly negative interaction.
5445.53 But what you're describing,
5446.7 it has so much more richness
and layers of detail
5450.131 that I can only imagine what
those relationships are like.
5453.71 - Well, there's a few,
just a quick comment.
5455.43 So, I've had, they're currently in Boston
5457.64 and I have a bunch of Roombas from iRobot.
5460.59 And I did this experiment-
5461.93 - Wait, how many Roombas?
5463.891 [Lex laughing]
5464.984 It sounds like a fleet of Roombas.
5465.96 - Yeah.
5467.037 So, it's probably seven or eight, yeah.
5469.03 - Well, it's a lot of Roombas.
5469.863 This place is very clean.
5472.82 - Well, so, this, I'm kind of waiting,
5474.78 this is the place we're
currently in Austin,
5478.46 is way larger than I need.
5481.48 But I basically got it to make
sure I have room for robots.
5487.21 - So, you have these seven or so Roombas,
5490.45 you deploy all seven at once?
5492.13 - Oh, no, I do different
experience with them,
5493.916 different experiments with them.
5496.06 So, one of the things I want
to mention, is this is a,
5498.87 I think there was a YouTube video
5500.18 that inspired me to try this,
5502.83 is I got them to scream
in pain and moan in pain
5508.77 whenever they were kicked or contacted.
5513.43 And I did that experiment
to see how I would feel.
5517.11 I meant to do like a YouTube video on it,
5518.84 but then it just seemed very cruel.
5520.4 - Did any Roomba rights
activists come at you?
5522.42 - Like I think if I released that video,
5527.66 I think it's going to make me look insane,
5529.73 which I know people
know I'm already insane-
5532.78 - Now, you have to release the video.
5533.866 [Lex laughing]
5534.699 - Sure.
5535.532 Well, I think maybe if I contextualize it
5538.33 by showing other robots
5540.42 like to show why this is fascinating,
5543.24 because ultimately,
5544.34 I felt like there were
human almost immediately.
5547.4 And that display of
pain was what did that.
5550.38 - Giving them a voice.
5551.53 - Giving them a voice,
5552.4 especially a voice of dislike, of pain.
5557.21 - I have to connect you
to my friend Eddie Chang,
5559.25 he studied speech and
language, he's a neurosurgeon,
5561.4 and we're life-long friends.
5563.79 He studied speech and language,
5566.39 but he describes some
of these more primitive,
5570.19 visceral vocalizations, cries,
groans, moans of delight,
5576.47 other sounds as well,
use your imagination,
5578.99 as such powerful rudders for the other,
5582.87 for the emotions of other people.
5584.143 And so, I find it fascinating,
5586.12 I can't wait to see this video.
5587.61 So, is the video available online?
5589.52 - No, I haven't recorded it,
5592.08 I just hit a bunch of Roombas
5593.71 that are able to scream in
pain in my Boston place.
5598.252 [Andrew laughing]
5599.41 So, like people already-
5602.37 - Next podcast episode with Lex,
5604.87 maybe we'll have that one, who knows?
5606.8 - So, the thing is like people,
5608.28 I've noticed because I
talk so much about love
5611.01 and it's really who I am,
5612.41 I think they want to,
5614.164 to a lot of people,
5615.72 seems like there's there
got to be a dark person
5618.57 in there somewhere.
5619.48 And I thought if I release
videos of Roombas screaming
5621.93 and they're like, yep, yep,
that guy's definitely insane.
5624.59 - What about like shouts
of glee and delight,
5627.81 you could do that too, right?
5628.78 - Well, I don't know how to,
5631.67 to me, delight is quiet, right?
5634.85 - You're a Russian.
5636.325 Americans are much louder than Russians.
5639.89 - Yeah.
- Yeah.
5640.723 - Yeah.
5641.556 But like I don't, unless
you're talking about like,
5644.37 I don't know
5645.203 how you would have sexual
relationships with the Roomba.
5647.12 - Well, I wasn't necessarily
saying a sexual delight, but-
5650.082 - Trust me, I tried.
5651.356 I'm just kidding, that's a joke, internet.
5654.13 Okay [giggles]
5654.963 but I was fascinated in the psychology
5656.81 of how little it took.
5657.75 'Cause you mentioned
5658.73 you had a negative
relationship with a Roomba.
5661.521 - Well, I'd find that mostly,
I took it for granted.
5664.93 - Yeah.
5665.763 - It just served me, it
collected Costello's hair.
5667.58 And then when it would do
something I didn't like,
5669.23 I would get upset with it.
5670.27 So, that's not a good relationship,
it was taken for granted
5673.41 and I would get upset and
then I'd park it again,
5676.09 and I just like you're in the corner.
5679.06 - Yeah.
5679.893 - But there's a way to
frame it being quite dumb
5685.41 as almost cute.
5687.72 You're almost connecting
with it for its dumbness.
5691.16 And I think that's artificial
intelligence problem.
5694.752 - [Andrew] It's interesting.
5695.585 - I think flaws should
be feature, not a bug.
5699.33 - So, along the lines of this,
5701.18 the different sorts of relationships
5702.59 that one could have with robots,
5703.607 and the fear, but also some
of the positive relationships
5706.94 that one could have,
5708.93 there's so much dimensionality,
there's so much to explore.
5712.44 But power dynamics in
relationships are very interesting,
5716.34 because the obvious ones,
5718.822 the unsophisticated view of this,
5721.04 is one, there's a master
and a servant, right?
5725.07 But there's also manipulation,
5727.84 there's benevolent manipulation,
5730.86 children do this with
parents, puppies do this.
5733.16 Puppies turn their head and look cute
5734.96 and maybe give out a little noise,
5737.6 kids coup and parents always
think that they're doing this
5741.58 because they love the parent,
5745.01 but in many ways,
5746.58 studies show that those coups are ways
5748.5 to extract the sorts of
behaviors and expressions
5750.86 from the parent that they want.
5751.85 The child doesn't know it's doing this,
5753.2 it's completely subconscious,
5754.29 but it's benevolent manipulation.
5756.5 So, there's one version of fear of robots
5759.75 that I hear a lot about
5761.043 that I think most people can relate to
5762.75 where the robots take over
5764.57 and they become the masters
and we become the servants.
5768.2 But there could be another version
5770.04 that in certain communities
5773.68 that I'm certainly not a part of,
5775.17 but they call topping from the bottom,
5777.25 where the robot is
actually manipulating you
5780.84 into doing things,
5782.39 but you are under the belief
that you are in charge,
5786.78 but actually they're in charge.
5789.24 And so, I think that's one
5791.0 that if we could explore
that for a second,
5794.56 you could imagine it
wouldn't necessarily be bad,
5797.19 although it could lead to bad things.
5800.17 The reason I want to explore this,
5801.12 is I think people always
default to the extreme,
5804.33 like the robots take over and
we're in little jail cells
5807.07 and they're out having fun
and ruling the universe.
5810.354 What sorts of manipulation
5813.05 can a robot potentially
carry out, good or bad?
5816.55 - Yeah.
5817.383 Just so, there's a lot of
good and bad manipulation
5819.83 between humans, right?
5820.81 Just like you said.
5823.354 [Lex sighing]
5824.44 To me [sighing] especially
like you said [giggles]
5829.97 topping from the bottom, is that the term?
5831.7 - I think someone from
MIT told me that term.
5834.551 [Lex laughing]
5835.815 It wasn't Lex.
5837.86 - I think.
5838.693 So, first of all, there's
power dynamics in bed,
5842.08 and power dynamics in relationships,
5844.25 and power dynamics on the street,
5845.9 and in the work environment,
those are all very different.
5849.359 I think power dynamics can
make human relationships,
5854.78 especially romantic relationships
fascinating and rich
5859.88 and fulfilling and exciting
and all those kinds of things.
5862.78 So, I don't think in
themselves they're bad,
5868.67 and the same goes with robots.
5870.86 I really love the idea
5872.106 that a robot will be at top or a bottom
5875.0 in terms of like power dynamics.
5877.54 And I think everybody
should be aware of that.
5880.02 And the manipulation is
not so much manipulation,
5882.49 but a dance of like pulling
away, a push and pull,
5886.74 and all those kinds of things.
5888.003 In terms of control,
5890.4 I think we're very, very,
very far away from AI systems
5893.95 that are able to lock us up.
5897.76 To lock us up in,
5900.23 like to have so much control
5901.67 that we basically cannot live our lives
5905.17 in the way that we want.
5906.95 I think there's a, in terms
of dangers of AI systems,
5909.73 there's much more dangers
5910.87 that have to do with
autonomous weapon systems
5913.087 and all those kinds of things.
5914.28 So, the power dynamics as
exercised in the struggle
5917.95 between nations and war and
all those kinds of things.
5920.7 But in terms of personal relationships,
5923.71 I think power dynamics
are a beautiful thing.
5925.95 Now, there's of course,
5927.21 going to be all those kinds of discussions
5929.61 about consent and rights and
all those kinds of things.
5932.66 - Well, here, we're talking about,
5934.0 I always say in any
discussion around this,
5936.47 if we need to define a really the context,
5939.21 it always should be
consensual, age-appropriate,
5943.32 context-appropriate, species-appropriate,
5946.11 but now we're talking about
human-robot interactions.
5949.25 And so, I guess that-
5950.82 - No, I actually was trying
to make a different point,
5953.66 which is I do believe
5955.02 that robots will have
rights down the line.
5957.88 And I think in order for us
5960.86 to have deep, meaningful
relationships with robots,
5963.1 we would have to consider
them as entities in themselves
5966.58 that deserve respect.
5969.77 And that's a really interesting concept
5971.277 that I think people are
starting to talk about
5974.38 a little bit more,
5975.52 but it's very difficult
for us to understand
5977.66 how entities that are other than human.
5979.75 I mean, the same as with
dogs and other animals
5982.51 can have rights on a level as humans.
5984.94 - Well, yeah.
5985.86 We can't and nor should we do
whatever we want with animals,
5989.98 we have a USDA, we have
Department of Agriculture
5994.74 that deal with animal care and
use committees for research,
5999.05 for farming and ranching and all that.
6002.11 So, when you first said it,
I thought, wait, why would,
6005.94 there'll be a bill of robotic rights,
6007.41 but it absolutely makes sense
6008.833 in the context of everything
we've been talking about
6012.23 up until now.
6015.293 If you're willing, I'd
love to talk about dogs,
6018.74 because you've mentioned
dogs a couple of times,
6021.31 a robot dog, you had a biological dog.
6026.08 Yeah.
- Yeah.
6026.913 I had a Newfoundland named
Homer for many years growing up.
6034.27 - In Russia or in the U.S.?
6035.64 - In the United States.
6037.33 And he was about, he's over
200 pounds, that's a big dog.
6040.97 - That's a big dog.
6042.04 - People know Newfoundland.
6043.76 So, he's this black dog
that's a really a long hair
6048.62 and just a kind soul.
6050.5 I think perhaps that's
true for a lot of large,
6053.27 but he thought he was a small dog.
6055.23 So, he moved like that, and-
6056.52 - Was he your dog?
6057.43 - Yeah, yeah.
6058.263 - So, you had him since
he was fairly young?
6060.89 - Since the very, very beginning,
till the very, very end.
6063.81 And one of the things, I
mean, he had this kind of a,
6068.66 we mentioned like the Roombas,
6071.33 he had kind-hearted dumbness about him
6075.156 that was just overwhelming,
6076.71 it's part of the reason I named him Homer,
6080.06 because it's after Homer Simpson,
6082.52 in case people are wondering
which Homer I'm referring to.
6087.041 [Andrew laughing]
6087.874 And so, there's a.
6088.97 Yeah, exactly.
6092.33 There's a clumsiness
that was just something
6095.24 that immediately led to a
deep love for each other.
6097.685 And one of the,
6100.685 I mean, he was always,
it's the shared moments,
6103.37 he was always there for
so many a nights together,
6106.55 that's a powerful thing about a dog
6108.64 that he was there through
all the loneliness,
6112.35 through all the tough
times, through the successes
6114.44 and all those kinds of things.
6115.87 And I remember,
6117.55 I mean, that was a really
moving moment for me,
6120.15 I still miss him to this day.
6122.19 - How long ago did he die?
6125.4 - Maybe 15 years ago.
6127.1 So, it's been awhile.
6129.17 But it was the first time
6131.964 I've really experienced
like the feeling of death.
6137.01 So, what happened, is he got a cancer.
6142.87 And so, he was dying slowly.
6146.06 And then there's a certain point
he couldn't get up anymore.
6149.396 Now, there's a lot of
things that could say here
6152.71 that I struggle with.
6154.184 Maybe he suffered much
longer than he needed to,
6159.12 that's something I
really think about a lot.
6162.08 But I remember when I had
to take him to the hospital
6167.2 and the nurses couldn't carry him, right?
6172.35 So, you're talking about a 200 pound dog
6174.12 and I was really into
power lifting at the time.
6176.73 And I remember they tried to figure out
6179.39 all these kinds of ways to.
6182.015 So, in order to put them to sleep,
6183.62 they had to take them into a room.
6187.27 And so, I had to carry him everywhere.
6189.54 And here's this dying friend
of mine that I just had to,
6195.26 first of all, that was
really difficult to carry,
6197.02 somebody that heavy when
they're not helping you out.
6199.791 And, yeah.
6202.76 So, I remember it was the first time
6205.96 seeing a friend laying there
6208.47 and seeing life drain from his body.
6214.138 And that realization that
we're here for a short time
6218.39 was made so real,
6220.59 that here is friend that was
there for me the week before,
6223.75 the day before, and now he's gone.
6226.27 And that was,
6228.5 I don't know, that spoke to the fact
6230.8 that he could be deeply
connected with the dog.
6234.56 Also spoke to the fact that
the shared moments together
6240.86 that led to that deep friendship
will make life so amazing,
6248.56 but it also spoke to the fact
that death is a motherfucker.
6253.44 So, I know you've lost Costello recently.
6255.87 - Yeah.
6256.872 - And you can-
- And as you're saying this,
6257.705 I'm definitely fighting back the tears.
6260.09 I thank you for sharing that.
6263.36 That I guess we're about to both cry over,
6266.19 I don't want to say dogs [laughing]
6268.69 that it was bound to happen
6270.34 just given when this is happening.
6273.53 Yeah, it's-
6275.266 - How long did you know that
Costello was not doing well?
6279.7 - We'll, let's see, a year
ago during the start of,
6284.31 about six months into the pandemic,
6286.48 he started getting
abscesses and he was not,
6289.11 his behavior change and
something really changed.
6291.62 And then I put him on testosterone
6296.09 which helped a lot of things.
6298.14 It certainly didn't cure everything,
6299.33 but it helped a lot of things,
6300.66 he was dealing with
joint pain, sleep issues,
6303.92 and then it just became
a very slow decline
6308.91 to the point where two, three weeks ago,
6311.41 he had a closet full of medication.
6315.37 I mean, this dog was,
it was like a pharmacy.
6317.39 It's amazing to me when I
looked at it the other day,
6319.6 I still haven't cleaned up
and removed all those things,
6322.12 'cause I can't quite bring
myself to do it, but-
6325.961 - Do you think he was suffering?
6327.51 - Well, so, what happened,
was about a week ago,
6330.33 it was really just about
a week ago, it's amazing.
6332.163 He was going up the
stairs, I saw him slip.
6335.08 And he was a big guy,
6335.913 he wasn't 200 pounds, but
he was about 90 pounds,
6337.86 he's a bulldog, he was
pretty big and he was fit.
6341.24 And then I noticed
6342.073 that he wasn't carrying a foot in the back
6344.45 like it was injured, it
had no feeling at all.
6346.44 He never liked me to touch
his hind paws, and I could do,
6348.92 that thing was just flopping there.
6350.52 And then the vet found
some spinal degeneration
6353.89 and I was told that the next one would go.
6355.76 Did he suffer?
6356.863 Sure, hope not, but something
changed in his eyes.
6360.96 - Yeah.
- Yeah, it's the eyes again,
6362.84 I know you and I spend
long hours on the phone
6365.16 and tell you about like the eyes
6366.41 and what they convey and what they mean
6368.05 about internal states
6369.23 and for sake of robots and
biology of other kinds, but-
6372.797 - You think something about
him was gone in his eyes?
6377.92 - I think he was real,
6380.66 here, I am anthropomorphizing,
6382.513 I think he was realizing that
one of his great joys in life,
6386.87 which was to walk and
sniff and pee on things.
6392.167 [Lex laughing]
6393.15 This dog.
6393.983 - The fundamental.
- Loved to pee on things,
6396.057 it was amazing.
6396.89 I've wondered where he put it,
6398.94 he was like a reservoir of
urine, it was incredible.
6402.33 I think, oh, that's Eddie,
6403.55 he'd put like one drop on
the 50 millionth plant.
6406.59 And then we get to the 50
millionth in one plant,
6409.017 and he just have, leave a puddle.
6411.26 And here I am talking
about Costello peeing.
6414.54 He was losing that ability
to stand up and do that,
6417.08 he was falling down
while he was doing that.
6418.92 And I do think he started to realize.
6421.6 And the passage was easy and peaceful,
6424.6 but I'll say this, I'm
not ashamed to say it,
6428.803 I mean, I wake up every
morning since then just,
6431.214 I don't even make the conscious decision
6433.27 to allow my self to cry, I wake up crying.
6436.11 And I'm unfortunately able
to make it through the day,
6438.2 thanks to the great support of my friends
6440.01 and you and my family,
but I miss him, man.
6443.847 - You miss him?
6444.68 - Yeah, I miss him.
6445.56 And I feel like, Homer, Costello,
6449.44 the relationship to one's
dog is so specific part.
6454.14 - So, that part of you is gone?
6457.97 - That's the hard thing.
6465.12 What I think is different,
6466.98 is that I made the mistake, I think.
6469.807 Moreover, I hope it was a good decision,
6471.85 but sometimes I think I made the mistake
6473.3 of I brought Costello a
little bit to the world
6476.52 through the podcast or posting about him,
6479.0 I anthropomorphized about him in public.
6481.58 Let's be honest, I have no
idea what his mental life was,
6483.73 or his relationship to me.
6484.91 And I'm just exploring all
this for the first time
6486.397 'cause he was my first dog,
6487.79 but I raised him since he was seven weeks.
6489.71 - Yeah, you got hold it together,
6491.05 I noticed the episode
you released on Monday,
6494.97 you mentioned Costello.
6496.61 Like you brought them back to life for me
6498.95 for that brief moment.
6500.35 - Yeah.
6501.401 But he's gone.
6502.42 - That's the,
6504.66 he's going to be gone
for a lot of people too.
6508.53 - Well, this is what I'm struggling with.
6509.86 I think that maybe you're
pretty good at this.
6513.42 Like, have you done this before?
6516.083 [Andrew laughing]
6517.977 This is the challenge.
6518.81 Is that actually part of me.
6520.6 I know how to take care
of myself pretty well.
6522.57 - Yeah.
6523.403 - Not perfectly, but pretty well.
6524.76 And I have good support.
6526.04 I do worry a little bit
about how it's going to land
6528.81 and how people will feel.
6530.23 I'm concerned about their internalization.
6534.121 So that's something
I'm still iterating on.
6536.3 - And you have to watch you struggle,
6538.4 which is fascinating.
6539.4 - Right.
6540.233 And I've mostly been
shielding them from this,
6541.66 but what would make me happiest
6544.01 is if people would internalize
6546.42 some of Costello's best traits.
6548.24 And his best traits were
that he was incredibly tough.
6554.26 I mean, he was a,
6555.84 22-inch-neck bulldog, the whole thing.
6557.72 He was just born that way.
6558.999 What was so beautiful
is that his toughness
6562.42 has never what he rolled forward.
6564.1 It was just how sweet and kind he was.
6567.14 And so if people can take that,
6569.18 then there's a win in there someplace, so.
6573.2 - I think there's some ways
6574.65 in which she should probably
live on in your podcast too.
6577.607 You should,
6579.1 I mean, it's such a,
6581.83 one of the things I loved
about his role in your podcast
6585.83 is that he brought so much joy to you.
6588.64 We mentioned the robots.
6589.925 - Mm-hmm.
6590.758 - Right?
6591.633 I think that's such a powerful thing
6594.35 to bring that joy into,
6596.33 like, allowing yourself
to experience that joy,
6599.42 to bring that joy to others,
to share it with others.
6602.48 That's really powerful.
6603.85 And I mean, not to,
6605.64 this is like the Russian thing,
6607.21 is [chuckles] it touched me
when Lucy Kay had that moment
6614.08 that I keep thinking about
in this show "Louie,"
6618.0 or like an old man was criticizing Louis
6620.29 for whining about breaking
up with his girlfriend,
6622.63 and he was saying like the most
beautiful thing about love,
6630.46 they made a song that's catchy
6632.34 now that's now making me
feel horrible saying it,
6635.69 but like, is the loss.
6637.74 The loss really also is making you realize
6641.26 how much that person,
6644.49 that dog meant to you.
6646.37 And like allowing
yourself to feel that loss
6648.74 and not run away from that
loss is really powerful.
6651.45 And in some ways that's also sweet,
6655.1 just like the love was
the loss is also sweet
6657.735 because you know that
you felt a lot for that
6662.085 through your friend.
6663.81 So I, and like continue to bring that joy.
6666.812 I think it would be
amazing to the podcast.
6670.435 I hope to do the same
with [laughing] robots,
6673.71 or whatever else is the
source of joy, right?
6677.75 And maybe, do you think about
one day getting another dog?
6682.49 - Yeah, in time.
6685.06 You're hitting on all
the key buttons here.
6688.61 I want that to,
6690.09 we're thinking about ways to
kind of immortalize Costello
6694.78 in a way that's real,
6695.66 not just creating some little
logo or something silly.
6699.95 Costello much like David
Goggins is a person,
6703.88 but Goggins also has
grown into kind of a verb.
6707.19 You're going to Goggins
this or you're going to,
6708.87 and there's an adjective.
6710.15 Like that's extreme.
6711.06 Like it,
6712.46 I think that for me, Costello
was all those things.
6714.49 He was a being, he was his own being,
6716.85 he was a noun, a verb and an adjective.
6720.38 So, and he had this amazing super power
6722.47 that I wish I could get,
6723.34 which is this ability to get everyone else
6725.09 to do things for you
without doing a thing.
6727.893 [Lex laughing]
6728.726 The Costello effect,
6729.83 as I call it.
6730.663 - So is an idea I hope he lives on.
6732.29 - Yes.
6733.81 Thank you for that.
6734.643 This actually has been
very therapeutic for me,
6738.1 which actually brings me to a question,
6742.38 we're friends, we're
not just co-scientists,
6745.87 colleagues working on a project together
6748.22 and in the world,
6753.33 that's somewhat similar.
6754.8 - Just two dogs.
6757.23 - Just two dogs basically.
6760.57 But let's talk about friendship,
6764.26 because I think that I
certainly know as a scientist
6769.19 that there are elements
that are very lonely
6771.15 of the scientific pursuit.
6772.78 There are elements of many
pursuits that are lonely.
6777.03 Music,
6778.35 Math always seem to me like they're like
6780.76 the loneliest people.
6782.125 Who knows if that's true or not.
6783.657 Also people work in teams.
6785.41 And sometimes people are surrounded
6786.63 by people interacting with
people and they feel very lonely.
6789.33 But for me, and I think as well for you,
6794.47 friendship is an incredibly strong force
6797.96 in making one feel like
certain things are possible
6803.56 or worth reaching for,
6805.82 maybe even making us
compulsively reach for them.
6808.25 So, when you were growing up,
6810.75 you grew up in Russia until what age?
6812.82 - 13.
6813.73 - Okay.
6814.563 And then you moved
directly to Philadelphia?
6818.32 - To Chicago.
6819.33 - [Andrew] Chicago.
6820.163 - And then Philadelphia,
6822.43 and San Francisco and Boston and so on.
6825.06 But really to Chicago, that's
where I went to high school.
6828.48 - Do you have siblings?
6829.88 - Older brother.
6830.73 - But most people don't know that.
6831.851 [Lex laughing]
6833.87 - Yeah, he is a very different person.
6838.01 But somebody I definitely look up to.
6839.64 So he's a wild man.
6840.81 He's extrovert,
6841.86 he was into, I mean,
6845.06 so he's also scientists, a bio engineer,
6847.58 but he's, when we were growing up,
6849.826 he was the person who did,
6854.91 drank and did every drug.
6856.92 And, but also as the life of the party.
6859.4 And I just thought he was the,
6861.12 when your older brother, five years older,
6863.48 he was the coolest person
that I was wanting to be him.
6868.34 So for that,
6869.92 he definitely had a big influence.
6871.23 But I think for me in terms
of friendship, growing up,
6875.926 I had one really close friend.
6880.09 And then when I came here
I had another close friend,
6882.33 but I'm very,
6884.36 I believe,
6886.255 I don't know if I believe,
6887.56 but I draw a lot of strength
from deep connections
6893.06 with other people,
6896.2 and just the small number of people.
6897.78 Just a really small number of people.
6899.09 That's when I moved to this country,
6900.53 I was really surprised how,
6902.29 like, there were
6903.366 these large groups of
friends, quote, unquote.
6908.33 But the depth of connection
was not there at all
6912.28 from my sort of perspective.
6914.3 Now, I moved to the suburb
of Chicago, was Naperville.
6917.74 It's more like a middle class,
maybe upper middle class.
6921.01 So it's like people that cared more
6923.58 about material possessions
than deep human connection.
6926.51 So that added to the thing.
6928.44 But I drove more meaning
than almost anything else
6934.05 was from friendship.
6935.05 Early on I had a best friend.
6937.22 His name was,
6938.41 his name is Yura.
6941.121 I don't know how to say it in English.
6943.47 - How do you say in Russian?
6944.72 - Yura.
6945.553 What's his last name?
6946.47 Do you remember if was...
6947.473 [Lex chuckles]
6948.93 - Mikolov.
6950.928 Yura Mikolov.
6953.75 So we just spent all our time together.
6956.37 There's also a group of friends.
6958.51 Like, I dunno, it's like eight guys.
6959.611 In Russia growing up,
6964.87 it's like parents didn't care
6967.51 if you're coming back at certain hour.
6969.7 So we'll spent all day, all
night just playing soccer,
6973.38 usually called football,
6976.006 and just talking about life
and all those kinds of things,
6978.94 even at that young age.
6980.82 I think people in Russia
6982.74 and Soviet Union grew up much quicker.
6985.234 [Lex chuckles]
6986.536 I think the education system
at the university level
6990.93 is world-class in the United
States in terms of like,
6994.59 really creating really
big, powerful minds.
6998.98 At least they used to be,
7000.09 but I think that they aspire to that.
7001.715 But the education system for like,
7004.81 for younger kids in the
Soviet Union was incredible.
7009.07 Like they did not treat us as kids.
7011.127 The level of literature,
Tolstoy, Dostoevsky.
7014.798 - When you were a small child?
7016.0 - Yeah.
7016.926 - Amazing.
7017.759 Amazing.
7018.592 - And like the level of mathematics,
7020.44 and you're made to feel like shit
7022.11 if you're not good at mathematics.
7023.81 Like we, I think in this country,
7026.46 there's more like,
7027.4 especially young kids
7028.24 'cause they're so cute.
7029.74 Like they're being babied.
7032.38 We only start to really
push adults later in life.
7035.28 Like, so if you want to be
the best in the world at this,
7037.89 then you get to be pushed.
7039.51 But we were pushed at a young age.
7041.81 Everybody was pushed.
7042.95 And that brought out the best in people.
7045.06 I think it really forced
people to discover,
7049.06 like discover themselves
in the Goggin style,
7051.89 but also discover what they're
actually passionate about,
7054.983 what they're not.
7055.816 - Is this true for boys and girls?
7057.49 Were they pushed equally there?
7059.01 - Yeah.
7059.843 They were pushed.
7060.676 Yeah, they were pushed
equally, I would say.
7061.99 There was a,
7063.03 obviously there was more,
7064.93 not obviously,
7065.763 but at least from my memories,
7068.63 more of a,
7070.4 what's the right way to put it?
7071.725 But there was like gender roles,
7073.91 but not in a negative connotation.
7076.65 It was the red dress
versus the suit and tie
7079.52 kind of connotation, which is like,
7081.6 there's a,
7084.63 like guys like lifting heavy things
7088.07 and girls like creating beautiful art,
7091.54 and, like there's-
7094.053 - A more traditional view of gender,
7095.733 more 1950, '60s.
7098.25 - But we didn't think in terms
of, at least at that age,
7100.44 in terms of like roles
and then like a homemaker
7103.83 or something like that or not,
7105.48 it was more about what people care about.
7108.27 Like girls cared about this set of things,
7111.42 and guys cared about the set of things.
7113.28 I think mathematics and engineering
7115.99 was something that guys
cared about and sort of,
7118.63 at least my perception of that time.
7120.53 And then girls cared about beauty.
7124.02 So like guys want to create machines,
7126.1 girls want to create beautiful stuff.
7128.035 [Lex laughing]
7128.882 And now, of course,
7129.89 that I don't take that forward
7132.79 in some kind of philosophy of life,
7134.413 but it's just the way I grew
up and the way I remember it.
7137.56 But all, everyone worked hard.
7141.63 The value of hard work was
instilled in everybody.
7146.53 And through that,
7150.11 I think it's a little bit of hardship.
7152.078 Of course also economically
everybody was poor,
7154.98 especially with the collapse
of the Soviet Union.
7157.14 There's poverty everywhere.
7158.65 You didn't notice it as much,
7159.84 but there was a,
7161.07 because there's not much
material possessions,
7164.14 there was a huge value
placed on human connection.
7167.673 Just meeting with neighbors,
7170.15 everybody knew each other.
7171.23 We lived in an apartment
building very different
7174.34 than you have in the
United States these days.
7176.5 Everybody knew each other.
7179.03 You would get together,
7179.96 drink vodka, smoke cigarettes,
7181.69 and play guitar and sing
sad songs about life.
7187.61 - What's with the sad
songs in the Russian thing?
7190.63 I mean, Russians do I express
joy from time to time.
7194.56 - Yeah, they do.
7195.97 - Certainly you do.
7197.59 But what do you think that's about?
7200.28 Is it 'cause it's cold there?
7201.36 But it's called other places too, right?
7204.49 - I think,
7205.44 let's just, first of all the Soviet Union,
7210.38 the echoes of World War II
and the millions and millions
7213.522 and millions of people that,
7214.72 civilians that were slaughtered,
7217.04 and also starvation is there, right?
7220.21 So like the echoes of that,
7223.213 of the ideas, the
literature, the art is there.
7225.95 Like that's a grandparents,
7227.34 that's parents, that's all there.
7229.37 So that contributes to it,
7230.6 that life can be absurdly
unexplainably cruel.
7235.62 At any moment everything can change.
7237.41 So that's in there.
7238.64 Then I think there's an empowering aspect
7240.96 to finding beauty in suffering
7243.27 that then everything
else is beautiful too.
7245.103 It's like, if you just
linger or it's like,
7247.68 why you meditate on death?
7249.28 Is like, if you just think
about the worst possible case
7252.08 and find beauty in that,
7253.13 then everything else is beautiful too.
7254.417 And so you write songs
about the dark stuff.
7257.44 And that's somehow helps you
deal with whatever comes.
7262.2 There's a hopelessness
to the Soviet Union that,
7267.49 like, inflation,
7269.39 all those kinds of things
where people were sold dreams
7273.24 and never delivered.
7275.44 And so like, there's a,
7278.22 if you don't sing songs about sad things,
7281.42 you're going to become
cynical about this world.
7284.0 - Mm-hmm.
7284.833 Interesting.
7285.692 - So they don't want
to give in to cynicism.
7287.39 Now, a lot of people did, one of the,
7291.54 but that is the battle against cynicism.
7294.28 One of the things that
may be common in Russia
7298.47 is a kind of cynicism about,
7301.07 like, if I told you the
thing I said earlier
7303.23 about dreaming about robots,
7304.853 it's very common for people
to dismiss that dream,
7308.84 of saying, no, that's
not, that's too wild.
7312.21 Like, who else do you know that did that?
7314.35 Or you want to start a podcast?
7315.92 Like who else?
7317.03 Like nobody's making money on podcasts.
7318.82 Like, why do you want to start a podcast?
7320.05 That kind of mindset I
think is quite common,
7323.41 which is why I would say
entrepreneurship in Russia
7327.55 is still not very good.
7329.36 Which to be a business,
7331.06 like, to be an entrepreneur
you have to dream big,
7333.47 and you have to have others around you,
7335.02 like friends and support
group that make you dream big.
7339.12 But if you don't give in to cynicism
7342.39 and appreciate the beauty
in the unfairness of life,
7347.83 the absurd unfairness of life,
7349.96 then I think it just makes you
appreciative of everything.
7354.52 It's like a,
7355.42 it's a prerequisite for gratitude.
7358.05 And so, yeah,
7360.34 I think that instilled in me ability
7362.58 to appreciate everything,
7364.02 just like everything.
7365.24 Everything's amazing.
7366.367 And then also there is a culture
7371.267 of like romanticizing everything.
7376.14 Like, it's almost like
romantic relationships
7381.37 were very like soap opera,
7383.43 like is very like over the top dramatic.
7387.72 And I think that it was
instilled in me too,
7390.93 not only do I appreciate
everything about life,
7393.68 but I get like emotional about it.
7395.616 In a sense, like,
7397.08 I get like a visceral feeling
of joy for everything.
7401.7 And the same with friends or
people of the opposite sex.
7406.13 Like, there's a deep,
7408.42 like emotional connection
there that like [laughing]
7413.0 that's like way too dramatic too.
7414.96 Like, I guess relative to
what the actual moment is.
7418.8 But I derive so much deep,
7422.38 like dramatic joy from
so many things in life.
7426.66 And I think I would attributed that
7428.94 to the upbringing in Russia.
7430.37 But the thing that sticks
most of all is the friendship.
7434.04 And have now since then had
one other friend like that
7439.244 in the United States, he lives in Chicago.
7442.66 His name is Matt.
7443.832 And slowly here and there accumulating
7448.32 really fascinating people,
7449.89 but I'm very selective with that.
7451.57 Funny enough, the few times,
7456.18 it's not few it's a lot of times now
7458.01 interacting with Joe Rogan [chuckles]
7459.94 it's sounds surreal to say,
7461.414 but there was a kindred spirit there.
7464.21 Like I've connected with him.
7466.811 And there's been people like that
7468.02 also in the grappling sports
that are really connected with.
7471.25 I've actually struggled,
7473.42 which is why I'm so
glad to be your friend,
7476.93 is I've struggled to
connect with scientists.
7480.29 - They can be a little
bit wooden sometimes.
7482.69 - [Lex] Yeah.
7483.523 - Even the biologist.
7484.356 I mean, one thing that I,
7486.559 well, I'm so struck by the
fact that you work with robots,
7490.13 you're an engineer, AI,
7491.408 science technology,
7492.98 and that all sounds like hardware, right?
7495.35 But what you're describing
and I know is true about you
7498.31 is this deep emotional
life and this resonance
7501.69 and it's really wonderful.
7502.74 I actually think it's one of
the reasons why so many people,
7506.53 scientists and otherwise
have gravitated towards you
7509.04 and your podcast is because
you hold both elements.
7512.1 In the Hermann Hesse's book,
7514.27 I don't know if you, "Narcissus
and Goldmund," right?
7516.14 It's about these elements
of the logical rational mind
7519.43 and the emotional mind and
how those are woven together.
7522.98 And if people haven't
read it, they should,
7524.9 and you embody the full picture.
7527.36 And I think that's so much
of what draws people to you.
7530.06 - I've read every Hermann
Hesse book by the way.
7531.96 - As usual [chuckles]
7533.49 as usual I've done about
9% of what life is.
7537.01 No, it's true.
7537.843 You mentioned Joe,
7539.74 who is a phenomenal human being,
7542.38 not just for his amazing accomplishments,
7544.3 but for how he shows up
to the world one on one.
7548.53 I think I heard him say the
other day on an interview,
7551.64 he said, there is no public
or private version of him.
7555.73 And he's like, this is me.
7557.03 He said that it was beautiful.
7558.35 He said, I'm like the fish
that got through the net.
7560.95 And there is no onstage offstage version.
7563.6 And you're absolutely right.
7564.61 And I.
7565.88 - Fish.
7566.713 [Lex laughing]
7567.68 - So, but, well, you guys,
7568.513 I have a question actually about-
7569.86 - But that's a really good point
7570.89 about public and private life.
7572.67 He was a huge,
7573.503 if I could just comment real quick.
7575.68 Like that, he was a,
7577.0 I've been a fan of Joe for a long time,
7578.61 but he's been an inspiration
7579.887 to not have any difference
between public and private life.
7584.74 I actually had a conversation
with Naval about this,
7588.2 and he said that you
can't have a rich life,
7594.29 like exciting life
7596.66 if you're the same person
publicly and privately.
7599.38 And I think I understand that idea,
7602.46 but I don't agree with it.
7605.41 I think that's really
fulfilling and exciting
7609.15 to be the same person
privately and publicly
7611.39 with very few exceptions.
7612.54 Now that said, I don't have
any really strange sex kinks.
7618.09 So like,
7618.923 I feel like it can be open
with basically everything.
7620.46 I don't have anything I'm ashamed of.
7623.58 There's some things that
could be perceived poorly,
7625.7 like the screaming Roombas,
7627.77 but I'm not ashamed of them.
7629.1 I just have to present
them in the right context.
7631.44 But there's a freedom to being
the same person in private
7635.62 as in public.
7636.67 And that Joe made me realize
that you can be that.
7642.5 And also to be kind to others.
7645.28 It sounds kind of absurd,
7648.67 but I really always enjoyed
like being good to others.
7658.49 Like just being kind towards others.
7661.13 But I always felt like the
world didn't want me to be.
7665.42 Like, there's so much negativity
when I was growing up,
7668.55 like just around people.
7669.74 If you actually just
notice how people talk,
7673.12 they, from like, complaining
about the weather,
7677.161 this could be just like the big cities
7679.01 that I've visited.
7679.843 But there's a general negativity,
7681.735 and positivity is kind of suppressed.
7685.07 You're not,
7685.903 one, you're not seen as very intelligent,
7688.75 and two, there's a kind of,
7690.45 you're seen as like a
little bit of a weirdo.
7692.86 And so I always felt
like I had to hide that.
7695.79 And what Joe made me realize,
7697.16 one, I could be fully
7699.13 just the same person private and public,
7702.33 and two, I can embrace being kind,
7705.18 in just, in the way that I like,
7708.31 in the way I know how to do.
7711.27 And sort of for me on like,
on Twitter or like publicly,
7716.43 whenever I say stuff,
7717.56 that means saying stuff simply
7719.67 almost to the point of cliche.
7721.24 And like, I have the
strength now to say it,
7724.59 even if I'm being mocked.
7726.157 [Lex laughing]
7726.99 You know what I mean?
7727.823 Like, just it's okay.
7728.72 If everything's going to be okay.
7730.43 Okay, some people will think you're dumb.
7732.071 They're probably right.
7733.56 The point is,
7734.393 like, just enjoy being yourself.
7736.32 And that Joe, more than
almost anybody else,
7738.35 because he's so successful at
it, inspired me to do that.
7743.1 Be kind and be the same
person, private and public.
7746.23 - I love it.
7747.063 And I love the idea that authenticity
7748.69 doesn't have to be oversharing, right?
7751.085 That it doesn't mean you reveal
every detail of your life.
7754.99 It's a way of being true
to an essence of oneself.
7759.06 - Right.
7759.98 There's never a feeling when
you deeply think and introspect
7764.46 that you're hiding
something from the world
7766.15 or you're being dishonest
in some fundamental way.
7768.77 So, yeah, that's truly liberating.
7773.52 It allows you to think,
7774.76 it allows you to like think freely,
7777.08 to speak freely,
7778.73 to just to be freely.
7782.14 That said,
7783.17 it's not like there's not
still a responsibility
7789.55 to be the best version of yourself.
7792.15 So, I'm very careful with
the way I say something.
7797.22 So, the whole point,
7798.65 it's not so simple to express the spirit
7802.61 that's inside you with words.
7804.997 I mean, some people are
much better than others.
7808.8 I struggle.
7809.633 Like oftentimes when I say something
7812.331 and I hear myself say it,
7814.25 it sounds really dumb and
not at all what I meant.
7816.75 So that's the responsibility you have.
7818.45 It's not just like being
7820.33 the same person publicly and privately
7821.87 means you can just say whatever the hell.
7824.5 It means there's still
responsibility to try to be,
7827.67 to express who you truly are.
7829.33 And that's hard.
7832.24 [Lex chuckles]
7833.073 - It hard.
7833.906 And I think that,
7834.739 we have this pressure,
7837.67 all people, when I say
we, I mean all humans,
7840.568 and maybe robots too,
7842.75 feel this pressure to be
able to express ourselves
7846.68 in that one moment in that one form.
7848.93 And it is beautiful when
somebody, for instance,
7851.3 can capture some essence
of love or sadness or anger
7855.19 or something in a song or in
a poem or in a short quote,
7858.79 but perhaps it's also possible
to do it in aggregate,
7863.21 all the things how you show up.
7865.658 For instance, one of the
things that initially drew me
7868.72 to want to get to know you as
a human being and a scientist
7871.46 and eventually we became friends,
7873.73 was the level of respect
7875.87 that you brought to your podcast listeners
7878.02 by wearing a suit.
7879.38 - Yeah.
- I'm being serious here.
7880.942 I was raised thinking that if
you overdress a little bit,
7884.39 overdressed by American,
certainly by American standards,
7887.3 you're overdressed for a podcast,
7888.68 but it's genuine.
7890.58 You're not doing it for any reason,
7891.86 except I have to assume,
and I assumed at the time,
7894.798 that it was because you have
a respect for your audience,
7897.99 you respect them enough to
show up a certain way for them.
7902.44 It's for you also, but it's for them.
7904.19 - Yeah.
7905.023 - And I think between that
7905.96 and your commitment to your friendship,
7907.893 just the way that you talk
about friendships and love
7910.07 and the way you hold
up these higher ideals,
7912.88 I think at least as a
consumer of your content
7917.94 and as your friend,
7919.617 what I find, is that in aggregate,
7921.8 you're communicating who you are,
7923.4 it doesn't have to be
one quote or something.
7925.85 And I think that we're sort of obsessed
7928.18 by like the one Einstein quote,
7930.16 or the one line of poetry or something,
7933.05 but I think you so embody the
way that, and Joe as well,
7938.32 it's about how you live your life
7940.04 and how you show up as
a collection of things
7942.88 and said and done.
7944.91 - Yeah, that that's,
7945.743 and so, the aggregate is
the goal, the tricky thing,
7950.42 and Jordan Peterson talks about this
7952.25 because he's under attack
7953.28 way more than you and I will ever be, but-
7956.26 - Right now?
7957.26 - For now, right?
7958.093 This is very true for now.
7960.47 That the people who
attack on the internet,
7966.99 this is one of the problems with Twitter,
7968.777 is they don't consider the aggregate,
7973.21 they take a single statements.
7975.34 And so, one of the defense mechanisms,
7978.46 like again why Joe has
been an inspiration,
7981.19 is that when you in
aggregate, a good person,
7985.57 a lot of people will know that.
7986.897 And so, that makes you much more immune
7988.98 to the attacks of people
7990.05 that bring out an individual statement
7991.627 that might be a misstatement of some kind,
7994.27 or doesn't express who you are.
7996.99 And so, that, I like that
idea is the aggregate.
8000.99 And the power of the podcast,
8003.82 is you have hundreds of hours out there,
8007.24 and being yourself and people
get to know who you are.
8010.202 And once they do,
8011.561 and you post pictures of screaming Roombas
8014.78 as you kick them,
8015.764 they will understand
that you don't mean well.
8018.16 By the way, as a side comment,
8021.32 I don't know if I want to release this
8022.97 because it's not just the Roombas-
8026.79 - You have a whole dungeon of robots.
8028.52 - Okay.
8029.353 So, this is a problem,
8031.65 the Boston Dynamics came
up against this problem.
8034.47 But, let me work this out like
workshop this out with you,
8039.87 and maybe because we'll post
this, people will let me know.
8045.59 So, there's legged robots,
they look like a dog,
8049.085 I'm trying to create a very
real human-robot connection,
8054.03 but like they're also incredible
8055.45 because you can throw them
like off of a building
8059.51 and they'll land fine.
8061.45 And this beautiful.
8062.43 - That's amazing.
8063.263 I've seen the Instagram videos
8064.36 of like cats jumping off of
like fifth story buildings
8067.32 and then walking away.
8068.69 But no one should throw their cat
8070.41 out of a window of a building.
8071.452 - Well, this is the
problem I'm experiencing,
8072.57 all certainly kicking the robots,
8074.72 its really fascinating how
they recover from those kicks.
8078.04 But like just seeing myself do it
8080.47 and also seeing others do it,
it just does not look good,
8083.4 and I don't know what to do with that.
8084.94 'Cause it's such a-
- Ill 'do it.
8088.093 [Lex laughing]
8089.43 - See.
8090.263 But you don't,
8091.91 'cause you are-
- A robot, no, I'm kidding.
8095.4 What's interesting?
8096.233 - Yeah.
8097.066 - Before today's conversation,
I probably could do it,
8100.02 and I'm thinking about robots,
bills of rights and things.
8104.611 Not to satisfy you or to satisfy anything,
8108.6 except that if they have
some sentience aspect
8113.04 to their being, then I
would load to kick it.
8116.44 - I don't think we'd be able to kick it,
8117.7 you might be able to kick the first time,
8119.03 but not the second, this is
the problem of experience.
8121.687 One of the cool things,
8122.875 is one of the robots I'm working with,
8126.61 you can pick it up by
one leg and is dangling,
8129.38 and you can throw it in any kind of way
8131.772 and it'll land correctly.
8133.71 So, it's really-
8134.704 - I had a friend who had a cat like that.
8135.72 [Lex laughing]
8137.778 - Oh man, we look forward
to the letters on the cat-
8140.6 - Oh no, I'm not
suggesting anyone did that,
8142.277 but he had this cat,
8143.97 and the cat, he would
just throw it onto the bed
8146.08 from across the room, and then
it would run back for more,
8148.84 or somehow that was the
nature of the relationship.
8151.62 I think no one should
do that to an animal,
8154.13 but this cat seemed to
return for whatever reason.
8157.58 - But a robot is a robot,
8158.777 and it's fascinating to me how
hard it is for me to do that.
8162.27 So, it's unfortunate,
8164.05 but I don't think I
can do that to a robot.
8166.28 Like I struggle with that.
8169.661 So, for me to be able
to do that with a robot,
8172.879 I have to almost get like into the state
8175.86 that I imagine like doctors get into
8177.75 when they're doing surgery,
8179.38 like I have to do what
robotics colleagues of mine do,
8183.02 which is like start
seeing it as an object.
8185.26 - Dissociate.
8186.093 - Like dissociate.
8186.926 So, it was just fascinating
8187.95 that I have to do that in
to do that with a robot.
8190.73 I just want to take that
little bit of a tangent.
8193.65 - No, I think it's an important thing.
8195.03 I mean, I'm not shy about
the fact that for many years
8200.71 I've worked on experimental animals,
8202.46 and that's been a very challenging aspect
8204.56 of being a biologist.
8206.1 Mostly mice, but in the past no longer,
8208.64 thank goodness 'cause I
just don't like doing it,
8211.25 larger animals as well.
8212.71 And now I work on humans,
8213.76 which I can give consent, verbal consent.
8215.95 So, I think that it's extremely important
8220.84 to have an understanding
of what the guidelines are
8223.92 and where one's own
boundaries are around this.
8226.22 It's not just an important question,
8229.06 it might be the most important question
8231.08 before any work can progress.
8232.91 - So, you asked me about friendship.
8234.96 I know you have a lot of
thoughts about friendship,
8238.21 what do you think is the
value of friendship in life?
8242.35 - Well, for me personally,
8244.85 just because of my life
trajectory and arc friendship,
8249.91 and I should say,
8251.54 I do have some female friends
that are just friends,
8255.65 they're completely platonic relationships,
8257.15 but it's been mostly male
friendship to me, has been-
8259.7 - It has been all male
friendships to me actually, yeah.
8261.96 - Interesting.
- Yeah.
8263.06 - It's been an absolute lifeline.
8265.73 They are my family,
8267.07 I have a biological family
8268.31 and I have great respect and love for them
8270.09 and an appreciation for them,
8271.29 but it's provided me the,
8277.912 I won't even say confidence
8278.92 because there's always an
anxiety in taking any good risk,
8281.997 or any risk worth taking.
8284.25 It's given me the sense that
I should go for certain things
8288.9 and try certain things to take
risk to weather that anxiety.
8292.21 And I don't consider myself
8294.67 a particularly competitive person,
8296.81 but I would sooner die than disappoint,
8301.91 or let down one of my friends.
8304.66 I can think of nothing worse actually,
8306.96 than disappointing one of my friends,
8309.17 everything else is secondary to me.
8311.29 - What disappointment?
8313.16 - Disappoint, meaning not,
8315.93 I mean, certainly I strive
always to show up as best I can
8320.76 for the friendship, and
that can be in small ways.
8323.0 That can mean making
sure the phone is away,
8325.03 sometimes it's about,
8328.58 I'm terrible with punctuality
'cause I'm an academic.
8330.86 And so, I just get lost in
time and I don't mean anything,
8333.27 but it's striving to, to listen to,
8336.32 to enjoy good times and to make time.
8338.54 It kind of goes back to this
first variable we talked about,
8341.89 to make sure that I spend time
8343.646 and to get time in person and check in.
8346.77 And I think there's so many ways
8350.69 in which friendship is vital to me,
8352.38 it's it's actually to me,
what makes life worth living.
8354.86 - Yeah.
8355.693 Well, there's a,
8357.43 I am surprised like with
the high school friends
8360.11 how we don't actually
talk that often these days
8362.361 in terms of time,
8363.78 but every time we see each other,
8364.98 it's immediately right
back to where we started.
8367.12 So, I struggled that how much
time you really allocate,
8372.62 for the friendship to be deeply meaningful
8374.43 because they're always there with me
8376.293 even if we don't talk often.
8379.56 So, there's a kind of loyalty.
8381.02 I think maybe it's a different style,
8384.04 but I think to me,
8387.04 friendship is being there
in the hard times, I think.
8391.64 Like I'm much more reliable
when you're going through shit
8396.52 than in like-
8397.61 - You're pretty reliable anyway.
8399.6 - No, but if you're like a
wedding or something like that,
8403.17 or like, I don't know, like
you want an award of some kind,
8408.77 yeah, I'll congratulate
the shit out of you,
8411.92 but like that's not, and I'll be there,
8414.5 but that's not as important
to me as being there
8416.74 when like nobody else
is like just being there
8420.28 when shit hits the fan,
or something's tough
8423.85 where the world turns their back on you,
8425.91 all those kinds of things,
8426.91 that, to me, that's where
friendship is meaningful.
8429.37 - Well, I know that to be true about you
8430.96 and that's a felt thing
and a real thing with you.
8433.049 Let me ask one more thing
about that actually,
8435.61 because I'm not a practitioner Jujitsu,
8438.46 I know you are, Joe is,
8439.7 but years ago, I read a
book that I really enjoyed,
8442.45 which is Sam Sheridan's
book, " A Fighter's Heart,"
8444.83 he talks about all these
different forms of martial arts.
8447.185 And maybe it was in the book,
maybe it was in an interview,
8450.68 but he said that fighting,
8453.33 or being in physical battle with somebody,
8456.24 Jujitsu boxing or some other form
8458.45 of direct physical contact
between two individuals
8461.87 creates this bond unlike any other.
8464.34 Because he said it's
like a one night stand,
8466.61 you're sharing bodily fluids
8468.06 with somebody that you barely know.
8469.83 - Yeah.
8470.663 - And I chuckled about it
'cause it's kind of funny
8473.14 and it kind of tongue in cheek.
8474.85 But at the same time,
8476.3 I think this is a fundamental way
8479.0 in which members of a species bond
8482.31 is through physical contact.
8484.35 And certainly, there are other forms,
8485.92 there's cuddling, and
there's hand holding,
8487.4 and there's in their sexual intercourse,
8489.71 and there's all sorts of things.
8490.543 - What's cuddling?
8491.579 I haven't heard of it.
8492.77 - I heard this recently,
I didn't know this term,
8494.91 but there's a term,
8496.18 they've turned the noun
cupcake into a verb,
8499.2 cupcaking it turns out, I
just learned about this.
8501.89 Cupcaking is when you
spend time just cuddling.
8505.1 I didn't know about this.
8506.34 You heard it here first,
8507.33 although I heard it
first just the other day.
8508.89 Cupcaking is actually a-
8510.03 - Cuddling is everything,
8510.92 it's not just like,
8511.81 is it in bed, or is it in the coach?
8513.79 Like what's cuddling?
8515.44 I do look up what cuddling is-
8516.39 - We need to look at this up
8517.223 and we need to define the variables.
8519.03 I think it definitely has to do
8520.67 with physical contact, I'm told,
8524.23 but in terms of battle, a competition,
8530.74 and the Sheridan quote, I'm just curious.
8532.66 So, do you get close, or
feel a bond with people that,
8538.39 for instance, you rolled Jujitsu with,
8540.13 or even though you don't know
anything else about them,
8544.01 was he right about this?
8545.54 - Yeah, I mean on many levels.
8547.17 He also has the book, what?
8548.357 "A Fighter's Mind."
8549.44 - Yeah, that was the third one.
8550.92 He's actually an excellent writer.
8552.17 What's interesting about him,
just briefly about Sheridan,
8554.54 I don't know him, but I did
a little bit of research,
8556.14 he went to Harvard, he was
an art major at Harvard,
8560.12 he claims all he did was
smoke cigarettes and do art.
8563.47 I don't know if his art was any good.
8565.14 And I think his father
was in the SEAL teams.
8568.348 And then when he got out
of Harvard, graduated,
8571.76 he took off around the world
8572.93 learning all the forms of martial arts,
8574.47 and was early to the
kind of ultimate fighting
8576.7 kind of mixed martial arts and things.
8579.05 Great book.
8580.0 Yeah, yeah.
8580.833 - It's amazing.
8581.666 I don't actually remember
it, but I read it,
8583.178 and I remember thinking that
was an amazing encapsulation
8586.667 of what makes fighting like the art,
8590.55 like what makes it compelling.
8591.98 I would say that there's so many ways
8595.82 that Jujitsu grappling, wrestling,
combat sports in general,
8600.825 is like one of the most
intimate things you could do.
8604.28 I don't know if I would describe
8605.6 in terms of bodily liquids
and all those kinds of things.
8607.643 - I think he was more or less joking.
8611.266 - I think there's a few
ways that it does that.
8615.1 So, one, because you're
so vulnerable [sighs]
8621.04 So, the honesty of stepping on the mat
8625.57 and often all of us have ego
8628.39 thinking we're better than we
are at this particular art.
8632.38 And then the honesty of being submitted,
8635.576 or being worse than you thought you are
8638.75 and just sitting with that knowledge,
8640.623 that kind of honesty,
8642.04 we don't get to experience
it in most of daily life.
8645.743 We can continue living
8646.956 somewhat of an illusion of
our conceptions of ourselves
8650.349 'cause people are not going
to hit us with the reality,
8653.79 the mat speaks only the truth,
the reality just hits you.
8657.86 And that vulnerability
8658.989 is the same as like the
loss of a loved one,
8662.16 though it's the loss of a
reality that you knew before,
8666.33 you now have to deal
with this new reality.
8668.21 And when you're sitting
there in that vulnerability,
8670.211 and there's these other people
8672.15 that are also sitting
in that vulnerability,
8674.35 you get to really connect like, fuck,
8676.96 like I'm not as special
as I thought I was,
8680.127 and life is like not, life is
harsher than I thought I was,
8686.202 and we're just sitting
there with that reality,
8687.71 some of us can put words
to them, some we can't.
8690.04 So, I think that definitely,
8691.3 is a thing that at least the intimacy.
8694.65 The other thing is the human contact.
8698.64 There's something about,
I mean, like a big hug,
8703.71 like during COVID,
8705.32 very few people hugged
me and I hugged them,
8707.81 and I always felt good when they did.
8710.28 Like we were all tested,
8711.61 and especially now we're vaccinated,
8713.89 but there's still people,
8715.12 this is true of San Francisco's,
it's true in Boston,
8717.36 they want to keep, not only six feet away,
8719.41 but stay at home and never touch you.
8721.76 That loss of basic humanity
8724.68 is the opposite of what I feel in Jujitsu,
8729.37 where it was like that
contact where you're like,
8733.621 I don't give a shit
8734.94 about whatever rules we're
supposed to have in society
8737.45 where you have to keep a distance
8739.42 and all that kind of stuff.
8740.48 Just the hug,
8741.86 like the intimacy of a hug,
that's like a good bear hug,
8746.47 and you're like just
controlling another person,
8749.75 and also there is some
kind of love communicated
8752.14 through just trying to
break each other's arms.
8754.205 I don't exactly understand
8756.024 why violence is the such
a close neighbor to love,
8761.24 but it is.
8762.227 - Well, in the hypothalamus,
8764.8 the neurons that control sexual behavior,
8768.27 but also non-sexual contact,
8771.018 are not just nearby the neurons
8773.56 that control aggression and fighting,
8775.81 they are salt and pepper
with those neurons.
8779.172 It's a very interesting,
8781.58 and it almost sounds
8783.16 kind of risky and controversial and stuff,
8785.07 I'm not anthropomorphizing
about what this means,
8787.87 but in the brain, those
structures are interdigitated,
8792.486 you can't separate them
except at a very fine level.
8796.04 And here, the way you describe it,
8797.75 is the same as a real thing.
8799.74 - I do want to make an
interesting comment.
8802.99 Again, these are the things
8803.823 that could be taken out of context,
8805.69 but one of the amazing
things about Jujitsu,
8810.57 is both guys and girls train it.
8813.14 And I was surprised.
8814.409 So, like I'm a big fan
of yoga pants [giggles]
8819.313 at the gym kind of thing.
8821.04 It reveals the beauty of the female form.
8824.81 But the thing is,
8826.24 like girls are dressed
in skintight clothes
8828.8 in Jujitsu often.
8830.21 And I found myself not at all,
8832.62 thinking like that at all
when training with girls.
8835.03 - Well, the context is very non-sexual.
8837.19 - But I was surprised to learn that.
8839.636 When I first started to Jujitsu,
8841.19 I thought, wouldn't that be kind of weird
8842.69 to train with the opposites
that in something so intimate.
8845.279 - So, boys and girls, men and women,
8847.88 they roll Jujitsu together?
8850.56 - Completely.
- Interesting.
8851.44 - And the only times girls kind
of try to stay away from guys,
8854.894 I mean, there's two contexts,
8856.44 of course, there's always going
to be creeps in this world.
8858.89 So, everyone knows who
kind of stay away from,
8862.27 and the other is there's a size disparity.
8864.31 So, girls will often
try to roll with people
8866.29 a little bit closer weight-wise,
8868.31 But no, that's one of the things
8871.13 that are empowering to women,
8872.689 that's what they fall in love with
8874.3 when they started doing Jujitsu,
8875.53 is first of all,
8877.31 they gain an awareness and
a pride over their body,
8880.27 which is great.
8881.103 And then second, they get to [chuckles]
8882.906 especially later on,
start submitting big dudes
8885.98 like these bros that come in
8889.28 who are all shredded and like muscular,
8891.33 and they get to technique to
exercise dominance over them,
8896.1 and that's a powerful feeling.
8897.364 - You've seen women force
a larger guy to tap her,
8901.57 or even choke them up.
8902.46 - Well, I was deadlifting like a four,
8909.53 oh boy, I think it's 495.
8911.58 So, I was really into power-lifting
8913.16 when I started at Jujitsu,
8915.03 and I remember being submitted by,
8917.44 I thought I walked in
feeling like I'm going to be,
8920.67 if not the greatest fighter
ever, at least top three.
8923.03 And so, as a white belt,
you roll in like all happy.
8927.45 And then you realize
8928.639 that as long as you're not
applying too much force,
8931.77 that you're having,
8932.603 I remember being submitted many times
8934.39 by like 130, 120-pound girls
8936.829 at our Balance Studios in Philadelphia,
8939.9 that a lot of incredible
female Jujitsu players.
8942.096 And that's really humbling too
8943.854 that technique can overpower
in combat pure strength.
8951.87 And that's the other thing,
8953.65 there is something about
combat that's primal.
8958.16 Like there, it just feels,
8962.07 it feels like we were born to do this.
8966.5 Like that-
8967.333 - We have circuits in our brain
8968.624 that are dedicated to
this kind of interaction.
8972.42 There's no question.
8974.11 - And that's what it felt like,
8975.45 it wasn't that I'm learning a new skill.
8978.96 It was like,
8979.793 somehow I am a remembering echoes
8982.84 of something I've learned in the past.
8984.44 - Well, it's like hitting puberty.
8985.69 A child before puberty has no concept
8987.8 of boys and girls having this attraction,
8991.04 regardless of whether
or not they're attracted
8992.65 to boys or girl, doesn't matter.
8993.69 At some point, most people, not all,
8995.63 but certainly, but most
people, when they hit puberty,
8998.4 suddenly people appear differently,
9000.991 and certain people take on a
romantic or sexual interest
9005.58 for the very first time.
9007.2 - Yeah.
9008.033 - And so it's like,
9008.866 it's revealing a circuitry in the brain.
9010.736 It's not like they learn that it's innate.
9014.36 And I think it,
9015.193 when I hear the way you describe Jujitsu
9018.45 and enrolling Jujitsu,
9019.95 it reminds me a little bit,
9021.11 Joe was telling me recently
9022.083 about the first time he went hunting
9024.47 and he felt like it revealed a circuit
9026.374 that was in him all along,
9028.56 but he hadn't experienced before.
9030.95 - Yeah.
9031.783 That's definitely there.
9032.616 And of course, there's
the physical activity.
9034.96 One of the interesting
things about Jujitsu
9037.443 is it's one of the really
strenuous exercises
9040.643 that you can do late into your adult life,
9043.92 like into your 50, 60, 70s, 80s.
9047.011 When I came up,
9049.22 there's a few people in
their 80s that were training.
9051.4 And as long as you're smart,
9053.13 as long as you practice techniques
9054.89 and pick your partners correctly,
9055.98 you can do that kind of art.
9057.34 That's late into life.
9058.173 And so you're getting exercise.
9059.57 There's not many activities I find
9061.92 that are amenable to that.
9064.52 So, because it's such a thinking game,
9068.17 the Jujitsu in particular is an art
9070.91 or technique pays off a lot.
9073.026 So you can still maintain,
9075.707 first of all, remain injury
free if you use good technique,
9080.86 and also through good
technique be able to go,
9085.93 be active with people that
are much, much younger.
9088.066 And so that was, to me,
9090.61 that and running are the two activities
9092.51 you can kind of do late in life.
9093.87 Because to me a healthy life
9096.23 has exercises as the piece of the puzzle.
9099.4 - No, absolutely.
9100.44 And I'm glad that we're
on the physical component,
9102.75 because I know that there's for you,
9107.99 you've talked before about the crossover
9109.71 between the physical and the
intellectual and the mental.
9115.03 Are you still running at
ridiculous hours of the night
9117.71 for ridiculously long?
9119.65 - Yeah, so, definitely.
9121.55 I've been running late
at night here in Austin.
9123.54 People tell,
9124.529 the area we're in now,
9125.84 people say it's a dangerous area,
9127.27 which I find laughable coming
from the bigger cities.
9130.63 No, I run late at night.
9132.83 There's something.
9135.2 - If you see a guy running
through Austin at 2:00 a.m.
9139.368 in a suit and tie, it's probably.
9140.201 [Lex laughing]
9142.25 - Well, yeah.
9143.083 I mean, I do think about that
9144.11 'cause I get recognized
more and more in Austin.
9146.67 I worry that,
9147.847 but not really,
9149.08 that I get recognized late at night.
9152.63 But there is something about the night
9156.65 that brings out those deep
philosophical thoughts
9159.0 and self-reflection, that really enjoy.
9160.76 But recently I started
getting back to the grind.
9164.59 So I'm going to be competing
9166.49 or hoping to be compete
in September and October.
9170.06 - In Jujitsu?
9170.893 - In Jujitsu, yeah.
9171.726 To get back to competition.
9172.782 And so that requires getting
back into a great cardio shape.
9178.63 I've been getting,
9179.87 running as part of my daily routine.
9182.34 - Got it.
9183.23 - [Lex] Yeah.
9184.215 - Well, I always know I can reach you
9185.18 regardless of time zone in
the middle of the night,
9187.69 wherever that happens.
9189.37 - Well, part of that has
to be just being single
9191.16 and being a programmer.
9193.589 Those two things just don't work well
9196.1 in terms of a steady sleep schedule.
9198.09 - It's not bankers hours kind of work.
9200.1 Nine to five.
9201.5 I want to, you mentioned single.
9203.31 I want to ask you a little bit
9204.88 about the other form of relationship,
9206.44 which is romantic love.
9209.89 So, your parents are still married?
9212.41 - Still married,
9213.243 still happily married.
9214.16 - That's impressive.
9214.993 - [Lex] Yeah.
9215.826 - A rare thing nowadays.
9216.66 - [Lex] Yeah.
9217.493 - So you grew up with that example?
9219.01 - Yeah.
9219.843 I guess that's a powerful thing, right?
9220.95 If there's an example
that I think can work.
9224.841 - Yeah.
9225.674 I didn't have that in my own family,
9226.54 but when I see it,
9229.785 it's inspiring and it's beautiful.
9232.07 The fact that they have that,
9233.247 and that was the norm for you,
9235.07 I think is really wonderful.
9236.9 - Well, it was a,
9238.04 in the case of my parents
it was interesting to watch
9240.12 'cause there's obviously tension.
9243.27 Like, there'll be times where they fought
9244.96 and all those kinds of things.
9246.752 They obviously get frustrated
with each other and they like,
9251.78 but they find mechanisms
9253.36 how to communicate that to each other,
9255.04 like to make fun of
each other a little bit,
9256.56 like to tease, to get some
of that frustration out,
9259.44 and then ultimately to reunite
9261.04 and find their joyful moments
9263.53 and be that the energy.
9265.33 I think it's clear
9266.6 'cause I got together in
there I think early 20s,
9268.87 like very, very young.
9269.913 I think you grow together as people.
9272.46 - Yeah.
9273.579 You're still in the critical
period of brain plasticity.
9275.504 [laughing]
9277.09 - And also, I mean,
9278.49 it's just like divorce was so frowned upon
9282.31 that you stick it out.
9283.532 And I think a lot of couples
especially from that time,
9285.92 the Soviet Union,
9286.753 that's probably applies
to a lot of cultures.
9288.324 You stick it out and you put in the work,
9290.67 you learn how to put in the work.
9292.36 And once you do,
9293.31 you start to get to
9294.23 some of those rewarding aspects of being,
9297.882 like through time has sharing
so many moments together.
9301.93 That's definitely something
that was an inspiration to me,
9307.86 but maybe that's where I have.
9309.74 So I have a similar kind of longing
9311.39 to have a lifelong partner,
9312.86 like to have that kind of view,
9314.94 where same with friendship,
9316.79 lifelong friendship is
the most meaningful kind.
9320.65 That there is something with that time
9322.09 of sharing all that time together.
9323.716 Like till death do us
part is a powerful thing.
9326.85 Not by force,
9327.69 not because of the religion said it
9329.25 or the government said it
or your culture said it,
9331.62 but because you want to.
9333.29 - Do you want children?
9334.56 - Definitely, yeah.
9335.96 Definitely want children.
9338.42 It's-
9339.253 - How many Roombas do you have?
9341.47 - Oh, I thought-
9342.303 - You should, no, no-
9343.136 - Human children?
9344.042 - No, human to human children.
9344.875 - 'Cause I already have the children.
9345.708 - Exactly.
9346.541 What I was saying,
9347.418 you probably need to at
least as many human children
9348.84 as you do Roombas.
9349.673 Big family, small family.
9353.184 - So.
9354.017 - In your mind's eyes,
9354.85 they're a bunch of
Fridman's running around.
9359.17 - So I'll tell you, like realistically,
9361.27 I can explain exactly my thinking,
9363.74 and this is similar to
the robotics work is,
9367.44 if I'm like purely logical right now,
9370.13 my answer would be I don't want kids.
9372.39 Because I just don't have enough time.
9375.76 I have so much going on.
9377.32 But when I'm using the
same kind of vision I use
9379.61 for the robots
9380.77 is I know my life will be
transformed with the first.
9385.03 Like I know I would love being a father.
9387.67 And so the question of how many,
9389.689 that's on the other side of that hill.
9392.961 It could be some ridiculous number.
9395.93 So I just know that-
9397.01 - I have a feeling and I could be,
9398.67 I don't have a crystal
ball, but I don't know.
9402.66 I see an upwards of,
9404.8 certainly three or more
come comes to mind.
9407.85 - So much of that has to do
9409.538 with the partner you're with too.
9411.96 So like that, that's
such an open question,
9415.127 especially in this society of
what the right partnership is.
9418.595 'Cause I'm deeply empathetic.
9422.85 I want to see, like to me,
9425.15 what I look for in your relationship
9426.63 is for me to be really excited
9429.81 about the passions of another person,
9432.11 like whatever they're into,
9433.07 it doesn't have to be a career success.
9435.77 Any kind of success,
9436.9 just to be excited for them,
9438.49 and for them to be excited for me.
9440.57 And like share in that
excitement and build,
9442.48 and build and build.
9443.399 But there was also
practical aspects of like,
9445.84 what kind of shit do you
enjoy doing together?
9448.677 And I think family is a
real serious undertaking.
9452.564 - It certainly is.
9454.07 I mean, I think that I
have a friend who said it,
9457.52 I think best,
9458.353 which is that you first have,
9461.26 he's in a very successful relationship
9463.02 and has a family.
9464.49 And he said,
9465.323 you first have to define the role
9467.51 and then you have to cast the
right person for the role.
9469.954 [Lex laughing]
9471.41 - Well, yeah, there's
some deep aspects of that,
9473.827 but there's also an aspect to
which you're not smart enough
9478.32 from this side of it to define the role.
9482.77 I think there's part of
it that has to be a leap
9484.87 that you have to take.
9486.55 And I see having kids that way.
9492.16 You just have to go with
it and figure it out also.
9496.15 As long as there's love there,
9497.68 like what the hell is life for even?
9500.62 So I've,
9501.9 there's so many incredibly
successful people that I know
9506.35 that I've gotten to
know that all have kids.
9509.0 And the presence of kids for the most part
9512.344 has only been something
that energizes them,
9516.64 something they gave them meaning,
9517.93 something that made them the
best version of themselves,
9520.24 like made them more productive, not less,
9522.42 which is fascinating to me.
9523.47 - It is fascinating.
9524.52 I mean, you can imagine
9525.353 if the way that you felt about Homer,
9527.29 the way that I feel
and felt about Costello
9529.79 is at all a glimpse of what
that must be like then.
9534.65 - Exactly.
9536.45 The downside,
9537.76 the thing I worry more about
is the partner side of that.
9544.61 I've seen,
9546.35 the kids are almost universally
9547.87 a source of increased productivity
and joy and happiness.
9551.75 Like, yeah, they're a pain in the ass.
9553.31 Yeah, is complicated.
9554.143 Yeah, so and so forth, people
like to complain about kids.
9557.48 But then when you actually look past
9559.65 that little shallow layer of
complaint, kids are great.
9562.98 The source of pain for a lot of people
9564.81 is if when the relationship doesn't work.
9567.62 And so I'm very kind of
concerned about like,
9572.64 dating is very difficult,
9573.94 and I'm a complicated person.
9576.6 And so it's been very difficult
9578.27 to find the right kind of person.
9581.575 But that statement doesn't even make sense
9585.08 because I'm not on dating
apps, I don't see people.
9588.3 You're like the first
person I saw in awhile.
9590.42 It's like you and Michael
Malice and like Joe.
9593.08 So, like, I don't think
I've seen like a female.
9599.88 What is it?
9600.89 An element of the female
species in quite a while.
9603.71 So, I think you have to
put yourself out there.
9606.56 What is it?
9607.48 Daniel Johnston says,
true love will find you,
9610.049 but only if you're looking.
9611.78 So there's some element
of really taking the leap
9613.72 and putting yourself out there
9614.72 in kind of different situations.
9617.03 And I don't know how to do that
9618.46 when you're behind a
computer all the time.
9620.5 - Well, you're a builder
and you're a problem solver,
9625.24 and you find solutions,
9629.05 and I'm confident the
solution is out there, and.
9634.569 - I think you're implying
9635.402 that I'm going to build the
girlfriend, which I think.
9638.568 - Well, and maybe we shouldn't
separate this friendship,
9643.057 the notion of friendship and community,
9645.5 and if we go back to this
concept of the aggregate,
9648.95 maybe you'll meet this
woman through a friend,
9652.132 or maybe you'll or something of that sort.
9653.653 - So, one of the things,
9655.253 I dunno if you feel the same way,
9656.95 I definitely one of those
people that just falls in love
9661.55 and that's it.
9662.51 - Yeah, I can't say I'm like that.
9664.03 With Costello it was instantaneous.
9666.62 - Yeah.
9667.453 - It really was.
9668.286 I mean, I know it's not romantic love,
9669.72 but it was instantaneous.
9670.77 No, I, but that's me.
9672.447 And I think that if you know, you know,
9674.72 because that's a good
thing that you have there.
9678.457 - It's, I'm very careful with that,
9681.78 because you don't want to fall
in love with the wrong person.
9684.88 So I try to be very kind of careful with,
9687.59 I've noticed this because
I fall in love with every,
9689.48 like this mug,
9690.56 everything I fall in love
with things in this world.
9694.05 So, like, you have to be really careful
9695.55 because a girl comes up to you and says
9701.08 she loves DUSTY HUSKY,
9703.929 that doesn't necessarily
mean to marry her tonight.
9706.74 - Yes.
9707.573 And I like the way you said that out loud
9709.1 so that you heard it,
9710.0 you doesn't mean you need
to marry her tonight, right?
9712.62 - [Lex] Exactly.
9713.453 - [Andrew] Right.
9714.447 - But I mean, but people are amazing,
9716.14 and people are beautiful and that's,
9718.19 so I'm fully embraced that,
9720.457 but also you have to be
careful with relationships.
9722.97 And at the same time,
9723.912 like I mentioned to you offline, I don't,
9727.84 there's something about me
9728.93 that appreciates swinging for
the fences and not dating,
9733.68 like doing serial
dating, or dating around.
9735.503 - Like you're a one guy,
one girl kind of guy.
9737.61 - [Lex] Yeah.
9738.443 - And you said that.
9739.276 - And it's tricky
9741.1 because you want to be careful
with that kind of stuff.
9743.9 Especially now there's a growing platform
9746.086 that have a ridiculous
amount of female interests
9749.19 of a certain kind.
9751.4 But I'm looking for deep connection,
9753.347 and I'm looking by sending home alone,
9756.163 and every once in a while
talking to Stanford professors.
9760.699 - Perfect solution.
- On a podcast.
9762.44 - Perfect solution.
9763.273 - Is going to workout great.
9764.29 - It's well,
9765.34 it's part of,
9767.3 that constitutes machine
learning of sorts.
9769.58 - Yeah, of sorts.
9771.13 - I do,
9771.963 you mentioned what has now
become a quite extensive
9775.89 and expansive public
platform, which is incredible.
9779.3 I mean, the number of people out,
9781.3 first time I saw your podcast,
9782.39 I noticed the suit,
9783.57 I was like, he respects his
audience, which was great,
9785.5 but I also thought this is amazing.
9788.055 People are showing up for
science and engineering
9790.79 and technology information
and those discussions
9792.85 and other sorts of discussions.
9794.16 Now, I do want to talk for
a moment about the podcast.
9798.14 So my two questions about the podcast are,
9801.76 when you started it, did you have a plan?
9804.34 And regardless of what that answer is,
9806.988 do you know where you're taking it,
9809.87 or would you like to leave us?
9811.758 I do believe in an element
of surprise is always fun.
9815.25 But what about the podcast?
9816.44 Do you enjoy the podcast?
9817.71 I mean, your audience
certainly includes me,
9820.38 really enjoys the podcast.
9821.76 It's incredible.
9822.67 - So I love talking to people,
9826.59 and there's something about microphones
9830.23 that really bring out the best in people.
9831.95 Like you don't get a
chance to talk like this.
9834.53 If you and I were just hanging out,
9836.13 we would have a very
different conversation
9837.724 in the amount of focus we
allocate to each other.
9841.91 We would be having fun
talking about other stuff
9844.42 and doing other things.
9845.726 There'll be a lot of distraction.
9847.52 There would be some phone use
and all that kind of stuff.
9851.07 But here we're 100% focused on each other
9854.02 and focus on the idea.
9856.05 And like sometimes playing with ideas
9858.24 that we both don't know the answer to,
9861.13 like a question we don't
know the answer to.
9863.12 We're both like fumbling with it,
9864.4 trying to figure out,
9865.53 trying to get some insights
9867.15 at something we haven't
really figured out before
9869.56 and together arriving at that.
9871.31 I think that's magical.
9872.44 I don't know why we need
microphones for that,
9874.24 but we somehow do.
9875.28 - It feels like doing science.
9876.78 - It feels like doing
science for me, definitely.
9878.76 That's exactly it.
9880.222 And I'm really glad you said that
9882.36 because I don't actually often say this,
9885.553 but that's exactly what I felt like.
9888.124 I wanted to talk to friends
and colleagues at MIT
9893.97 to do real science together.
9896.46 That's how I felt about it.
9897.83 Like to really talk through problems
9899.83 that are actually interesting,
9903.15 as opposed to like incremental work
9906.1 that we're currently working for
9909.51 for a particular conference.
9910.89 So really asking questions like,
9912.32 what are we doing?
9914.17 Like, where's this headed to?
9916.03 Like, what are the big,
9917.24 is this really going to help us solve,
9919.89 in the case of AI, solve intelligence?
9922.83 Like, is this even
working on intelligence?
9924.835 There's a certain sense,
9926.41 which is why I initially called
it artificial intelligence.
9930.02 Is like most of us are not working
9932.94 on artificial intelligence.
9934.32 You're working on some
very specific problem
9937.72 and a set of techniques,
9939.207 at the time it's machine learning
9941.31 to solve this particular problem.
9943.01 This is not going to take us to a system
9945.061 that is anywhere close
to the generalizability
9949.25 of the human mind.
9951.37 Like the kind of stuff
the human mind can do
9952.98 in terms of memory, in terms of cognition,
9954.7 in terms of reasoning,
9955.73 common sense reasoning.
9956.98 This doesn't seem to take us there.
9958.73 So the initial impulse was,
9960.57 can I talk to these folks
9962.541 do science together through conversation?
9965.239 And I also thought that
there was not enough,
9971.61 I didn't think there was
enough good conversations
9973.389 with world-class minds that I got to meet.
9977.7 And not the ones with the book,
9979.85 or like this was just the thing.
9981.79 Oftentimes you go on this
tour when you have a book,
9984.3 but there's a lot of minds
that don't write books.
9986.95 - And the books constrain
the conversation too,
9988.714 when you're talking about
this thing, this book.
9991.77 - But there's,
9992.66 I've noticed that,
9994.03 with people that haven't written
a book who are brilliant,
9997.29 we get to talk about ideas in a new way.
10000.11 We both haven't actually,
10002.65 when we raise a question,
10003.81 we don't know the answer to it
once the question is raised.
10007.13 And we try to arrive there.
10009.013 Like, I dunno,
10010.7 I remember asking questions
of world-class researchers
10015.09 in deep learning of,
10018.26 why do neural networks
work as well as they do?
10022.64 That question is often loosely asked,
10026.66 but like when you have microphones
10028.938 and you have to think through it,
10031.04 and you have 30 minutes to an hour
10032.5 to think through it together,
I think that's science.
10036.15 I think that's really powerful.
10037.66 So that was the one goal.
10039.72 The other one is,
10043.546 again, don't usually talk about this,
10045.5 but there's some sense in which I wanted
10048.54 to have dangerous conversations.
10052.49 Part of the reasons I
wanted to wear a suit
10055.5 is like, I want it to be fearless.
10057.941 The reason I don't usually talk about it
10060.22 is because I feel like I'm
not good at conversation.
10063.04 So it looks like it doesn't
match the current skill level.
10068.28 But I wanted to have really
dangerous conversations
10073.9 that I uniquely would be able to do.
10078.21 Not completely uniquely,
10080.32 but like, I'm a huge fan of Joe Rogan,
10082.5 and I had to ask myself,
10084.57 what conversations can I
do that Joe Rogan can't?
10087.932 For me, I know I bring this up,
10092.01 but for me that person I thought about
10093.8 at the time was Putin.
10095.69 Like that's why I bring him up.
10097.57 He's just like with Costello,
10100.49 he's not just a person.
10102.11 He's also an idea to me
for what I strive for.
10105.63 Just to have those
dangerous conversations.
10107.9 And the reason I'm uniquely qualified
10110.67 as both the Russian,
10111.503 but also there's the judo
and the martial arts,
10114.08 there's a lot of elements that
make me have a conversation
10117.85 he hasn't had before.
10119.168 And there's a few other people
10123.2 that I kept in mind, like Don Knuth,
10126.6 is a computer scientist from Stanford
10128.968 that I thought is one of the
most beautiful minds ever.
10134.09 And nobody really talked to him,
10137.32 like really talked to him.
10139.54 He did a few lectures, which people love,
10141.42 but really just have a
conversation with him.
10143.327 There's a few people like that.
10144.85 One of them passed away, John Conway,
10146.75 that I never got,
10147.583 we agreed to talk, but
he died before we did.
10150.69 There's a few people like that,
10152.0 that I thought like it's such a crime
10155.73 to not hear those folks.
10159.45 And I have the unique ability
10162.52 to know how to purchase
a microphone on Amazon
10166.88 and plug it into a
device that records audio
10169.48 and then publish it, which
seems relatively unique.
10172.09 Like that's not easy in
the scientific community.
10174.75 People knowing how to
plug in a microphone.
10176.74 - No.
10177.573 They can build Faraday cages,
10178.96 and two-photon microscopes
and bioengineer,
10182.24 all sorts of things,
10183.08 but the idea that you could take ideas
10186.42 and export them into a
structure or a pseudo structure
10189.32 that people would benefit from
10190.71 seems like a cosmic achievement to them.
10194.09 - I don't know if it's fear
10195.53 or just a basically they haven't tried it,
10198.193 so they haven't learned the skill level.
10200.37 - But I think they're not trained.
10201.64 I mean, we could riff on this for awhile,
10203.41 but I think that,
10205.5 but it's important.
10206.37 And maybe we should, which is that it's,
10208.928 they're not trained to do it.
10211.09 They're trained to think in specific games
10212.86 and specific hypotheses,
10214.3 and many of them don't care to, right?
10217.979 They became scientists because
that's where they felt safe,
10222.71 and so why would they
leave that Haven of safety?
10225.92 - Well, they also don't
necessarily always see
10227.9 the value in it.
10228.8 We're all together learning,
10230.64 you and I are learning the value of this.
10233.78 I think you're probably having
10236.01 an exceptionally successful
and amazing podcast
10239.233 that you started just recently.
10240.95 - Thanks to your encouragement.
10242.47 - Well, but there's a
raw skill there that's,
10247.02 you're definitely an inspiration to me
10249.14 in how you did the podcast
10250.437 in the level of excellence you reach.
10252.88 But I think you've discovered
10254.114 that that's also an
impactful way to do science.
10257.08 That podcast.
10258.18 And I think a lot of scientists
10260.24 have not yet discovered that.
10262.053 That this is a,
10263.25 if they apply same kind of rigor
10266.9 as they do to academic publication
10268.82 or to even conference presentations,
10271.487 and they do that rigor
and effort to podcast,
10276.18 whatever that is,
10277.013 that could be a five-minute podcast,
10278.55 a two-hour podcasts,
10279.61 it could be conversational,
10280.717 or it can be more like lecture like,
10282.96 if they apply that effort,
10284.128 you have the potential to reach over time,
10286.87 tens of thousands, hundreds of thousands,
10288.64 millions of people.
10289.907 And that's really, really powerful.
10292.51 But yeah, for me giving a
platform to a few of those folks,
10299.44 especially for me personally,
10300.99 so maybe you can speak to
what fields you're drawn to,
10306.3 but I thought computer scientists
10311.2 were especially bad at this.
10313.59 So there's brilliant computer scientists
10316.33 that I thought it would be
amazing to explore their mind,
10320.32 explore their thinking.
10322.09 And so that I took that almost as an,
10325.19 on as an effort.
10326.58 And at the same time I
had other guests in mind,
10331.21 or people that connect
to my own interests.
10333.94 So the wrestling.
10336.84 Wrestling, music, football,
10338.93 both American football and soccer.
10341.13 I have a few particular people
10342.65 that I'm really interested in.
10344.781 Buvaisar Saitiev.
10346.567 The Saitiev brothers,
even Khabib for wrestling,
10349.68 just to talk to them, 'cause.
10350.91 - Oh, 'cause you can,
10351.75 you guys can communicate-
10353.05 - In Russian and in wrestling, right?
10356.42 As wrestlers and as Russians.
10358.88 And so that little,
10361.79 it's like an opportunity to explore a mind
10363.763 that I'm able to bring to the world.
10367.72 And also it,
10370.46 I feel like it makes me a better person,
10373.404 just that being that vulnerable
10375.67 and exploring ideas together.
10377.41 I don't know, like good conversation.
10379.85 I don't know how often
10380.683 you have really good
conversation with friends,
10382.27 but like podcasts are like that.
10383.991 And it's deeply moving.
10386.49 - It's the best.
10387.96 And what you brought through.
10389.97 I mean, when I saw you
sit down with Penrose,
10392.23 Nobel Prize winning physicist,
10393.98 and these other folks that,
10395.06 it's not just 'cause he has a Nobel,
10396.34 it's what comes out of
his mouth is incredible.
10398.07 And what you were able to
hold in that conversation
10402.94 was so much better,
10404.44 light years beyond what he
had any other interviewer,
10408.91 I don't want to even
call you an interviewer
10410.14 'cause it's really about conversation.
10411.65 Light years beyond what anyone else
10413.24 had been able to engage with him
10416.76 was such a beacon of what's possible.
10420.0 And I know that,
10420.97 I think that's what people are drawn to.
10422.48 And there's a certain intimacy,
10424.45 that certainly to people,
our friends, as we are,
10427.58 and they know each other,
10428.63 that there's more of that,
10429.9 but there's an intimacy
10431.38 in those kinds of private conversations
10433.23 that are made public.
10434.99 And.
10435.92 - Well, that's the, with you,
10437.94 you're probably starting
to realize, and Costello,
10441.23 is like, part of it,
10443.51 because you're authentic
10444.6 and you're putting yourself
out there completely,
10446.726 people are almost not just consuming
10450.165 the words you're saying,
10453.33 they also enjoy watching you, Andrew,
10457.03 struggle with these ideas
10459.13 or try to communicate these ideas.
10460.74 They like the flaws,
10461.75 they like a human being.
10463.64 - Oh, good, that flaws.
10464.93 - Well, that's good 'cause
I got plenty of those.
10466.26 - But they like the self-critical aspects,
10468.7 like where you're very careful,
10470.36 where you're very
self-critical about your flaws.
10472.796 I mean, in that same way,
10474.9 it's interesting I think for people
10476.25 to watch me talk to Penrose,
10477.99 not just because Penrose
is communicating ideas,
10482.33 but here's this like silly
kid trying to explore ideas.
10487.0 Like they know this kid.
10488.396 There's a human connection
that is really powerful.
10491.27 Same, I think with Putin, right?
10493.61 Like it's not just as a
good interview with Putin,
10497.49 it's also, here's this kid struggling
10500.2 to talk with one of the most powerful,
10504.81 some will argue dangerous
people in the world.
10508.3 They love that.
10509.24 The authenticity that led up to that.
10512.084 And in return,
10514.2 I get to connect,
10515.14 everybody I run to in the street
10516.84 and all those kinds of things,
10519.068 there's a depth of connection there,
10521.06 almost within like a minute or
two that's unlike any other.
10524.35 - Yeah, there's an intimacy
10525.27 that you've formed with with them.
10526.88 - Yeah, we've been on this
like journey together.
10529.77 And yeah, I have the
same thing with Joe Rogan
10531.33 before I ever met him, right?
10532.394 Like I was,
10533.83 because I was a fan of
Joe for so many years,
10536.107 there's something,
10538.42 there's a kind of friendship
as absurd as it might be
10542.67 to say in podcasting and
listening to podcasts.
10545.99 - Yeah.
10546.823 Maybe it fills in a little bit of that
10548.9 or solves a little bit of that loneliness
10550.69 that you've been talking about earlier.
10551.523 - Until the robots are here.
10553.11 [laughing]
10554.55 - I have just a couple more questions,
10556.57 but one of them is on behalf
of your audience, which is,
10561.65 I'm not going to ask you
the meaning of the hedgehog,
10564.51 but I just want to know,
10566.44 does it have a name?
10568.5 And you don't have to tell us the name,
10569.95 but just, does it have a name?
10571.01 Yes or no?
10572.035 - Well, there's a name he
likes to be referred to as,
10577.55 and then there's a private name
10579.33 in the privacy of his own
company that we call each other.
10581.32 No.
10582.365 [Lex laughing]
10583.32 I'm not that insane.
10584.4 No, his name is Hedgy.
10587.32 He's a hedgehog.
10588.67 I don't like stuffed animals.
10591.75 But his story is one of minimalism.
10595.63 So I gave away everything I own,
10599.25 now three times in my life.
10601.15 By everything I mean almost everything,
10603.0 kept jeans and shirt and a laptop.
10606.17 And recently it's also been guitar,
10609.14 things like that.
10611.29 But he survived because he was always in,
10615.36 at least in the first two
times was in the laptop bag,
10618.59 and he just got lucky.
10620.34 And so I just liked the
perseverance of that.
10623.02 And I first saw him in the,
10626.0 the reason I got a stuffed animal,
10627.34 I don't have other stuffed animals,
10629.63 is it was in a thrift store,
10633.01 in this like giant pile of stuffed animals
10636.05 and he jumped out at me,
10638.13 because unlike all the rest of them,
10640.05 he has this intense mean look about him.
10645.27 That he's just,
10646.9 he's upset at life,
10649.64 at the cruelty of life.
10650.94 And it's just,
10651.773 especially in the contrast
of the other stuffed animals,
10653.393 they have this dumb smile on their face.
10655.89 If you look at most stuffed animals,
10657.344 they have this dumb look on their face.
10658.75 They're just happy.
10659.68 Is like "Pleasantville."
10660.59 - It's what we say in neuroscience,
10661.7 they have a smooth cortex,
10663.015 not many form.
10664.47 - Exactly.
10665.303 And this,
10666.136 like Hedgy like saw through all of it.
10668.16 He was like Dustyesky's
man from underground.
10672.04 I mean, there's a sense
10672.92 that he saw the darkness of
the world and persevered.
10676.6 So like,
10677.433 and there's also a famous Russian cartoon,
10680.667 "Hedgehog in the Fog" that I grew up with,
10683.95 I connected with.
10684.95 [Lex laughing]
10685.81 People who know of that cartoon,
10687.63 you could see it on YouTube, it's.
10689.657 - "Hedgehog in the Fog."
10690.83 - Yeah.
10691.733 [Lex laughing]
10693.83 It's just, as you would expect,
10696.108 especially from like
early Soviet cartoons.
10697.94 It's a hedgehog, like sad,
10701.1 walking through the fog,
10702.91 exploring like loneliness and sadness.
10705.34 It's like, but it's beautiful.
10706.81 It's like a piece of art,
10707.87 people should,
10708.703 even if you don't speak Russian,
10709.63 you'll see, you'll understand.
10711.61 - Oh, the moment you said
that I was going to ask,
10713.97 so it's in Russian?
10714.803 But of course it's in-
10715.636 - It's in Russian, but it's more,
10717.37 it's very little speaking in it.
10719.06 It's almost a,
10721.18 there's an interesting exploration
10723.09 of how you make sense of the world
10727.41 when you see it only
vaguely through the fog.
10732.2 So he's trying to understand the world.
10735.47 - We have Mickey Mouse,
10736.757 we have Bugs Bunny.
10738.77 We have all these crazy animals,
10741.347 and you have the "Hedgehog in the Fog."
10743.61 - So there's a certain period,
10745.55 and this is again,
10746.727 I don't know what it's attributed to,
10749.29 but it was really powerful,
10750.427 which there's a period in Soviet history,
10752.99 I think probably '70s and '80s where like,
10758.42 especially kids were
treated very seriously.
10761.33 Like they were treated
like they're able to deal
10764.35 with the weightiness of life.
10767.85 And that was reflected in the cartoons.
10770.123 And there was,
10772.15 it was allowed to have like
really artistic content,
10777.22 not like dumb cartoons
10778.64 that are trying to get you
to like smile and run around,
10781.01 but like create art.
10782.35 Like stuff that,
10783.65 you know how like short cartoons
10785.35 or short films can win Oscars,
10786.636 like that's what they're swinging for.
10788.74 - So what strikes me
about this is a little bit
10791.14 how we were talking
about the suit earlier,
10792.81 it's almost like they
treat kids with respect.
10795.16 - Yeah.
10795.993 - Like that they have an intelligence
10798.32 and they honor that intelligence.
10799.79 - Yeah, they're really
just adult in a small body.
10803.4 Like you want to protect them
10804.65 from the true cruelty of the world.
10806.15 - Sure.
10806.983 - But in terms of their
intellectual capacity
10808.49 or like philosophical capacity,
10810.29 they are right there with you.
10811.56 And so that the cartoons reflected that,
10813.914 the art that they consumed,
10816.06 education reflected that.
10817.75 So he represents that.
10819.15 I mean, there's a sense of,
10822.62 because it's survived so long
10824.73 and because I don't like stuffed animals,
10827.37 that it's like we've been
through all of this together
10830.95 and it's the same,
10831.93 sharing the moments
together as the friendship.
10834.38 And there's a sense in which,
10836.25 if all the world turns
on you and goes to hell,
10839.04 at least we got each other.
10841.22 And he doesn't die,
10842.43 because he's an inanimate object, so.
10845.52 - Until you animate him.
10847.24 - Until you animate him.
10849.1 And then I probably would want to know
10850.47 what he was thinking
about this whole time.
10853.52 He's probably really into Taylor Swift
10855.24 or something like that.
10856.073 And it's like that I wouldn't
even want to know anyway.
10859.55 - Well, I now feel a connection to Hedgy,
10861.84 the hedgehog that I
certainly didn't have before.
10864.15 And I think that encapsulates
10865.4 the kind of possibility of connection
10869.67 that is possible between
human and other object
10873.66 and through robotics certainly.
10877.77 There's a saying that I heard
when I was a graduate student
10880.0 that's just been ringing in my mind
10882.56 throughout this conversation in such a,
10885.1 I think appropriate way, which is that,
10888.61 Lex, you are in a minority of one,
10891.35 you are truly extraordinary
in your ability to encapsulate
10896.75 so many aspects of science, engineering,
10900.21 public communication,
about so many topics,
10903.428 martial arts and the emotional
depth that you bring to it.
10907.16 And just the purposefulness,
10909.09 and I think if it's not clear to people,
10911.68 it absolutely should be stated.
10913.93 But I think it's abundantly clear
10915.49 that just the amount of time
10917.82 and thinking that you put into things is,
10921.42 it is the ultimate mark of respect.
10924.85 So, I'm just extraordinarily
grateful for your friendship
10928.07 and for this conversation.
10929.19 - I'm proud to be a friend.
10931.18 And I just wished you showed me
10932.44 the same kind of respect by wearing a suit
10934.34 and make your father
proud maybe next time.
10936.596 [Andrew laughing]
10937.53 - Next time indeed.
10939.11 Thanks so much my friend.
10940.49 - Thank you.
10941.323 Thank you, Andrew.
10942.39 - Thank you for joining
me for my discussion
10944.57 with Dr. Lex Fridman.
10946.32 If you're enjoying this
podcast and learning from it,
10949.18 please consider subscribing on YouTube.
10951.4 As well, you can subscribe
to us on Spotify or Apple.
10955.185 Please leave any questions
and comments and suggestions
10958.25 that you have for future
podcast episodes and guests
10960.88 in the comment section on YouTube.
10963.27 At Apple, you can also leave
us up to a five-star review.
10966.82 If you'd like to support this podcast,
10968.49 we have a Patreon.
10969.67 That's patreon.com/andrewhuberman.
10972.86 And there you can support us
at any level that you like.
10976.24 Also, please check out
our sponsors mentioned
10978.57 at the beginning of the podcast episode.
10980.89 That's the best way to
support this podcast.
10983.24 Links to our sponsors can
be found in the show notes.
10986.8 And finally, thank you for
your interest in science.
10990.532 [bright music]
